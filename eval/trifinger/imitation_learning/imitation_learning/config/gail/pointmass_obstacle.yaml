# @package _global_
defaults:
  - override /env: pointmass_obstacle
  - override /evaluator: pointmass

evaluator:
  plot_il: False
 
num_steps: 50
num_envs: 128
log_interval: 10
save_interval: 10000000000
eval_interval: 400
num_env_steps: 1e7

policy_updater:
  _target_: gail.updater.GAIL
  _recursive_: False

  dataset_path: data/traj/pm_obstacle_stable_actions_100.pth
  batch_size: 256
  reward_update_freq: 1
  num_discrim_batches: 2
  num_envs: ${num_envs}

  discriminator:
    _target_: gail.discriminator.GailDiscriminator
    obs_shape: ${obs_shape}
    action_dim: ${action_dim}
    reward_hidden_dim: 128
    n_hidden_layers: 2
    cost_take_dim: -1
    use_actions: True
    reward_type: "GAIL"

  policy_updater:
    _target_: policy_opt.ppo.PPO
    _recursive_: False

    use_clipped_value_loss: True
    max_grad_norm: -1
    value_loss_coef: 0.5
    clip_param: 0.2
    entropy_coef: 0.001
    num_epochs: 2
    num_mini_batch: 4

    # Returns calculation
    gae_lambda: 0.95
    use_gae: True
    gamma: 0.99
    optimizer_params:
      _target_: torch.optim.Adam
      lr: 3e-4
    num_steps: ${num_steps}
    num_envs: ${num_envs}

  discrim_opt:
    _target_: torch.optim.Adam
    lr: 3e-4

eval_args:
  policy_updater:
    reward_update_freq: -1
    n_inner_iters: 1
  load_policy: False
  num_env_steps: 5e6

