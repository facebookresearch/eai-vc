# @package _global_
defaults:
  - override /env: pointmass
  - override /evaluator: pointmass

evaluator:
  plot_il: False
num_steps: 5
num_envs: 256
log_interval: 10
eval_interval: 100
num_env_steps: 1e6
info_keys : ["dist_to_goal", "r", "max_action_magnitude", "avg_action_magnitude", "final_obs"]

policy:
  _target_: imitation_learning.policy_opt.sac_policy.Policy
  hidden_size: 128
  recurrent_hidden_size: 128
  is_recurrent: False
  obs_shape: ${obs_shape}
  action_dim: ${action_dim}
  action_is_discrete: ${action_is_discrete}
  std_init: -1.0
  squash_mean: True
  num_envs: ${num_envs}

#TODO: check params
policy_updater:
  _target_: imitation_learning.policy_opt.sac.SAC
  _recursive_: False

  entropy_coef: 0.0
  use_clipped_value_loss: True
  clip_param: 0.2
  value_loss_coef: 0.5
  max_grad_norm: -1
  num_epochs: 2
  num_mini_batch: 4
  num_envs: ${num_envs}
  num_steps: ${num_steps}

  alpha_init: 1.0
  min_alpha: 0.1
  max_alpha: 10.0
  fixed_alpha: False

  # Returns calculation
  gae_lambda: 0.95
  use_gae: True
  gamma: 0.99
  optimizer_params:
    lr: 3e-4


        
    