defaults:
  - _self_
  - wandb_mae@: mae_eaif 

batch_size: 64                 # Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus)
epochs: 400
accum_iter: 1                   # Accumulate gradient iterations (for increasing the effective batch size under memory constraints)

# Model parameters
mae_model: mae_vit_small_patch16    # Name of model to train
input_size: 224                 # images input size
vip: True
use_mask: False
mask_ratio: 0.75                # Masking ratio (percentage of removed patches)
norm_pix_loss: False            # Use (per-patch) normalized pixels as targets for computing loss
use_cls: True 
global_pool: False 
# Optimizer parameters
weight_decay: 0.05
lr:                             # absolute lr
blr: 1e-3                       # base learning rate: absolute_lr = base_lr * total_batch_size / 256 (use either lr or blr)
min_lr: 0.0                     # lower lr bound for cyclic schedulers that hit 0
warmup_epochs: 40               # epochs to warmup LR

# Dataset parameters
# data_path: ["/datasets01/imagenet_full_size/061417/"]                 # The data_path can point to multiple datasets
data_path: ["/checkpoint/maksymets/eaif/datasets/ego4d"]
output_dir: "./output_dir"      # path where to save, empty for no saving
device: cuda                    # device to use for training / testing
seed: 0
resume: ""                      # resume from checkpoint
start_epoch: 0
num_workers: 10
pin_mem: True                   # Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.

# distributed training parameters
world_size: 1
local_rank: -1
dist_on_itp: False
dist_url: "env://"               # url used to set up distributed training"
distributed: True

# dataset arguments ** NEW **
dataset_type: path_dataset        # choices [dataset_with_txt_files, omnidataset, path_dataset],
omnidata_datasets: all           # Which omnidata datasets to use
dataset_size: 12m                # choices=["14_5m", "3_6m", "1_45m", "145k"],
color_jitter: False              # apply color jitter as a part of transforms