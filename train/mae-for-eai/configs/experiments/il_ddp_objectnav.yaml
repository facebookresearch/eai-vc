# Note:  This is an example config, see habitat_baselines/config/pointnav/ppo_pointnav.yaml
# for better hyperparameters for actual trainingsem_seg_pred

BASE_TASK_CONFIG_PATH: "configs/tasks/objectnav_hm3d_il.yaml"
TRAINER_NAME: "ddp-il-trainer"
ENV_NAME: SimpleRLEnv
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
VIDEO_OPTION: []
TENSORBOARD_DIR: "tb/objectnav_mae/objectnav_il/overfitting/sem_seg_pred/seed_1/ckpt_20"
VIDEO_DIR: "video_dir/objectnav_mae/objectnav_il/overfitting/sem_seg_pred/seed_1/ckpt_20"
# To evaluate on all episodes, set this to -1
TEST_EPISODE_COUNT: -1
EVAL_CKPT_PATH_DIR: "data/new_checkpoints/objectnav_mae/objectnav_il/overfitting/sem_seg_pred/seed_1/ckpt.20.pth"
SHOW_TOP_DOWN_MAP: False
NUM_PROCESSES: 4
CHECKPOINT_FOLDER: "data/new_checkpoints/objectnav/objectnav_il/overfitting/sem_seg_pred/seed_1/"
OUTPUT_LOG_DIR: data/objectnav_mae/logs
LOG_INTERVAL: 10
LOG_METRICS: True
CHECKPOINT_INTERVAL: 500
SENSORS: ['RGB_SENSOR'] #, 'DEPTH_SENSOR']
RESULTS_DIR: "data/objectnav_mae/results/objectnav_il/overfitting/sem_seg_pred/{split}/{type}"
EVAL_RESUTLS_DIR: "data/objectnav/results/"
EVAL_SAVE_RESULTS: True
EVAL_SAVE_RESULTS_INTERVAL: 50
EVAL:
  SPLIT: "val"
  meta_file: "tb/objectnav_mae/objectnav_il/overfitting/sem_seg_pred/seed_1/ckpt_20/evaluation_meta.json"
NUM_UPDATES: 16000
TOTAL_NUM_STEPS: -1.0
NUM_CHECKPOINTS: -1
WANDB:
  GROUP_NAME: "SemSeg"
  PROJECT_NAME: "objectnav_il"
  JOB_TYPE: "train"
  MODE: "online"
  RESUME: "must"
  TAGS: ["cross_entropy"]
  LOG_DIR: "wandb/"

IL:
  POLICY:
    name: "ObjectNavILPolicy"
  USE_IW: True
  distrib_backend: GLOO
  BehaviorCloning:
    lr: 0.001
    eps: 1.0e-5
    wd: 1.0e-6
    clip_param: 0.2
    num_mini_batch: 2
    max_grad_norm: 0.2
    num_steps: 64
    use_linear_clip_decay: False
    use_linear_lr_decay: True
    reward_window_size: 50
    sync_frac: 0.6

    pretrained: False
    pretrained_weights: "None"

RL:
  SUCCESS_REWARD: 2.5
  SLACK_REWARD: -1e-3

  REWARD_MEASURE: "distance_to_goal"

MODEL:
  RGB_ENCODER:
    cnn_type: "VisualEncoder"
    backbone: "vit_small_patch16"
    pretrained_encoder: "data/visual_encoders/mae_vit_small_decoder_large_HGPS_RE10K_100.pth"
  DEPTH_ENCODER:
    cnn_type: "None" #"VlnResnetDepthEncoder"
    output_size: 128
    backbone: "resnet50"
    trainable: False
    ddppo_checkpoint: "data/ddppo-models/gibson-2plus-resnet50.pth"
  STATE_ENCODER:
    hidden_size: 2048
    rnn_type: "GRU"
    num_recurrent_layers: 2
  SEQ2SEQ:
    use_prev_action: True
  CRITIC:
    no_critic: True
    mlp_critic: False
    hidden_dim: 512
