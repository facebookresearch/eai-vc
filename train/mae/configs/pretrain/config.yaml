defaults:
  - _self_
  - wandb_mae@: mae_eaif 
  - dataset: ego_inav

batch_size: 64                  # Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus)
epochs: 400
accum_iter: 1                   # Accumulate gradient iterations (for increasing the effective batch size under memory constraints)

# Model parameters
mae_model: mae_vit_large_patch16    # Name of model to train
# mae_model: mae_vit_base_patch16    # Name of model to train
input_size: 224                 # images input size
mask_ratio: 0.75                # Masking ratio (percentage of removed patches)
norm_pix_loss: False            # Use (per-patch) normalized pixels as targets for computing loss

# Optimizer parameters
weight_decay: 0.05
lr:                             # absolute lr
blr: 1e-3                       # base learning rate: absolute_lr = base_lr * total_batch_size / 256 (use either lr or blr)
min_lr: 0.0                     # lower lr bound for cyclic schedulers that hit 0
warmup_epochs: 40               # epochs to warmup LR

output_dir: "./output_dir"      # path where to save, empty for no saving
device: cuda                    # device to use for training / testing
seed: 0
resume: ""                      # resume from checkpoint
start_epoch: 0
num_workers: 10
pin_mem: True                   # Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU.

# distributed training parameters
world_size: 1
local_rank: -1
dist_on_itp: False
dist_url: "env://"               # url used to set up distributed training"
distributed: True

omnidata_datasets: all           # Which omnidata datasets to use
color_jitter: False              # apply color jitter as a part of transforms

# submitit

ngpus: 8
nodes: 2
timeout: 4320
partition: learnlab
# partition: devlab
use_volta32: False
comment: ""

# params filled by submitit
gpu: 0
rank: 0
dist_backend: nccl
