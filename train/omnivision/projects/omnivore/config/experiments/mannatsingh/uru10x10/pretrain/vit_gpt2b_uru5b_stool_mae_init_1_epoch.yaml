# @package _global_

scratch:
  # number of classes in the `stool` version of Uru 5B
  num_classes: 28051
  batch_size: 32
  num_samples: 5000000000
  phases_per_epoch: 100

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: ${scratch.phases_per_epoch}
  accelerator: cuda
  seed_value: 123
  mode: train

  data:
    _target_: omnivision.data_module.base_data_module.BaseDataModule
    train:
      _target_: omnivore.data.airstore_dataset.AirStoreTorchDataLoader
      dataset:
        _target_: omnivore.data.airstore_dataset.AirstoreImageDataset
        table_name: omniscale_uru10x10_caption
        total_length: ${scratch.num_samples}
        data_column: image
        label_column: labels
        id_column: NULL
        phases_per_epoch: ${scratch.phases_per_epoch}
        drop_last: True
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  interpolation: 3
                - _target_: torchvision.transforms.RandomHorizontalFlip
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: label
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                # "c1,c2,...,cN" --> torch.Tensor([c1, c2, ..., cN], dtype=int)
                - _target_: omnivision.utils.generic.csv_str_to_int_tensor
                  _partial_: True
                # (N,) --> (1, N)
                - _target_: torch.unsqueeze
                  dim: 0
                  _partial_: True
                # (1, N) --> (N, C) 1-hot vector
                - _target_: omnivision.utils.generic.convert_to_one_hot
                  classes:  ${scratch.num_classes}
                  is_one_idexed: True
                  _partial_: True
                # (N, C) to (C,)
                - _target_: torch.sum
                  dim: 0
                  _partial_: True
                # Need a float output to work with BCELoss
                - _target_: omnivision.utils.generic.change_dtype
                  dtype: "float"
                  _partial_: True
      batch_size: ${scratch.batch_size}
      num_workers: 6
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: image
      worker_init_fn: NULL

  model:
    _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
    trunk:
      _target_: omnivore.models.vision_transformer.VisionTransformer
      patch_size: 14
      embed_dim: 2560
      depth: 24
      drop_path_rate: 0.0
      attn_target:
        _target_: omnivore.models.vision_transformer.Attention
        _partial_: true
        num_heads: 32
        proj_drop: 0
        qk_scale: null
        qkv_bias: true
        attn_drop: 0
    heads:
    - head:
        _target_: omnivision.model.model_init_utils.init_parameters
        model:
          _target_: torch.nn.Sequential
          _args_:
          - _target_: torch.nn.Linear
            in_features: 2560
            out_features: 2560
          - _target_: torch.nn.Tanh
          - _target_: torch.nn.Linear
            in_features: 2560
            out_features: ${scratch.num_classes}
        init_fns:
          0.weight:
            _target_: omnivore.models.helpers.lecun_normal_init
            _partial_: true
            fan_in: 2560
          0.bias:
            _target_: torch.nn.init.zeros_
            _partial_: true
          2.weight:
            _target_: torch.nn.init.zeros_
            _partial_: true
          2.bias:
            _target_: torch.nn.init.zeros_
            _partial_: true
      fork_module: ''
      input_key: null
      output_key: null
    trunk_fields:
    - input_key: null
      args:
      - vision
  optim:
    amp:
      enabled: true
      amp_dtype: bfloat16
    optimizer:
      _target_: torch.optim._multi_tensor.AdamW
      weight_decay: 0.1
      # Uru 10x10 used a beta2 of 0.95 which is more stable. beta2 of 0.999
      # results in slightly better results.
      # This setting works till at least 2B parameters.
      betas: [0.9, 0.999]
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              # we use a linear LR decay schedule with linear warmup
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 4e-5
                end_value: 4e-4
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 4e-4
                end_value: 0
            # warm up for 5% of training
            lengths: [0.05, 0.95]
            interval_scaling: ['rescaled', 'rescaled']

  meters:
    train:
      image:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5

  loss:
    image:
      _target_: omnivore.losses.soft_target_cross_entropy_loss.SoftTargetCrossEntropyLoss

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir: ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 25  # save checkpoints every 25% of training
    model_weight_initializer:
      _partial_: true
      _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
      strict: false # heads aren't loaded
      state_dict:
        _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: /checkpoint/omniscale_ugc/mannatsingh/omnivision_omnivore/config/experiments/mannatsingh/mae/pretrain/vit_gpt_2b_uru5b_1_epoch_bs_4k_gpus_128.yaml/0/checkpoints/checkpoint.pt
        ckpt_state_dict_keys:
        - model
        checkpoint_kernels:
        - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
          key_pattern:
          - trunk.decoder.*
          # we do not exclude the norm for MAE-init
          # - "trunk.norm.*"
          - trunk.mask_token
          - heads.*

launcher:
  num_nodes: 32
  gpus_per_node: 8
  experiment_log_dir: ???

submitit:
  partition: learnlab
  constraints: NULL
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 32
  port_range: [10000, 65000]
