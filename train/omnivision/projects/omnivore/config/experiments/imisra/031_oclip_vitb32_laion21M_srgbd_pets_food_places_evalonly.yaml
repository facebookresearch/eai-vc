# @package _global_

defaults:
  - /experiments/imisra/base_v0_ampbf16
  - /experiments/imisra/base_dataset_paths

constants:
  rgb_crop_size: 224

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 32
  mode: val
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.vision_text_dataset.VisionTextDataset
        base_dataset:
          _target_: omnivore.data.webdataset_helpers.WebVisionTextPipeline
          base_dataset_length: 21e6
          base_dataset_fn:
            _target_: omnivore.data.webdataset_helpers.get_wds_dataset
            _partial_: True
            resampled: True # needed for multi-node training
            urls:
              _target_: omnivore.utils.data.FileLoader.load
              return_idx: False
              path_list:
                - /checkpoint/imisra/datasets/laion/laion400m_subset21M_tarlist.pkl
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: ${constants.rgb_crop_size}
                  interpolation: 3
                  scale: [0.9, 1.0]
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: ${constants.in1k_rgb_mean}
                  std: ${constants.in1k_rgb_std}
          - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
            base_transform:
              _target_: slip.tokenizer.SimpleTokenizer
              bpe_path_list: ${constants.bpe_path_list}
      shuffle: True
      batch_size: 256
      num_workers: 12
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      # ImageNet-1K
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.in1k_val_path_file_list}
          label_file_list: ${constants.in1k_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      # SUN RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_image_only
        worker_init_fn: NULL
      # NYUv2 RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_image_only
        worker_init_fn: NULL
      # Places365
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.places365_val_path_file_list}
          label_file_list: ${constants.places365_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: places365
        worker_init_fn: NULL
      # # Food101
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101_openaifoodtemplates
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101_openaiin1ktemplates
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101_sliptemplates
        worker_init_fn: NULL
      # # Pets
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets_openaipetstemplates
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets_openaiin1ktemplates
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 12
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets_sliptemplates
        worker_init_fn: NULL
  model:
    _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
    state_dict:
      _target_: omnivision.model.checkpoint_utils.load_checkpoint
      pick_recursive_keys: ["model"]
      path_list:
      - /fsx-omnivore/imisra/omnivision_omnivore/config/experiments/imisra/023_oclip_vitb32_laion21M_imval_b32K_msclip_params_ep32.yaml/0/checkpoints/checkpoint.pt
    model:
      _target_: omnivore.models.openclip_model.MultiModalZeroShotEvalWrapperCLIP
      image_output_key: image_embed
      text_output_key: text_embed
      logit_scale_output_key: logit_scale
      clip_model:
        _target_: omnivore.models.openclip_model.CLIP
        embed_dim: 512
        vision_cfg:
          timm_model_name: NULL
          timm_model_pretrained: False
          timm_pool: NULL
          timm_proj: NULL
          image_size: 224
          patch_size: 32
          width: 768
          layers: 12
        text_cfg:
          context_length: 77
          vocab_size: 49408
          width: 512
          heads: 8
          layers: 12
      label_strings:
        in1k:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list: ${constants.in1k_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list: ${constants.in1k_zs_templates_list}
        sunrgbd_image_only:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.sun_zs_classnames_list}
          templates: ${..in1k.templates}
        nyuv2scene_image_only:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_multi_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.nyuv2scene_zs_classnames_list}
          templates: ${..in1k.templates}
        places365:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.places365_zs_classnames_list}
          templates: ${..in1k.templates}
        food101_openaifoodtemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates: ${constants.food101_openaifood_zs_templates_list}
        food101_openaiin1ktemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates: ${..in1k.templates}
        food101_sliptemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            sub_key: "food101"
            path_list: ${constants.all_slip_templates_list}
        pets_openaipetstemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates: ${constants.pets_openaipets_zs_templates_list}
        pets_openaiin1ktemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates: ${..in1k.templates}
        pets_sliptemplates:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${..in1k.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            sub_key: "pets"
            path_list: ${constants.all_slip_templates_list}
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-6
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1.6e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1.6e-4
            lengths: [0.00625, 0.99375]  # warm for 2440 iters; 1 epoch = 305 iter
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
              - '*.bias'
              - '*.positional_embedding'
              - '*.class_embedding'
              - "*.logit_scale"
          module_cls_names: ['omnivore.models.openclip_model.LayerNorm']

  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
      sunrgbd_image_only: ${.in1k}
      nyuv2scene_image_only: ${.in1k}
      places365: ${.in1k}
      food101_openaiin1ktemplates: ${.in1k}
      food101_openaifoodtemplates: ${.in1k}
      food101_sliptemplates: ${.in1k}
      pets_openaiin1ktemplates: ${.in1k}
      pets_openaipetstemplates: ${.in1k}
      pets_sliptemplates: ${.in1k}
  loss:
    in1k:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: ${trainer.model.model.image_output_key}
      feat2_name: ${trainer.model.model.text_output_key}
      logit_scale_name: ${trainer.model.model.logit_scale_output_key}
      normalize: False # OpenClip normalizes outputs in the model

launcher:
  num_nodes: 1
  gpus_per_node: 1
