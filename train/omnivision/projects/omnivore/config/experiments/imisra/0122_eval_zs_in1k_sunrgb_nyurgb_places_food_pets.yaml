# @package _global_

defaults:
  - /experiments/imisra/base_dataset_paths

constants:
  rgb_crop_size: ???
  sunrgbd_interp_int: 2

trainer:
  data:
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      # ImageNet-1K
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.in1k_val_path_file_list}
          label_file_list: ${constants.in1k_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      # SUN RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_image_only
        worker_init_fn: NULL
      # NYUv2 RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_image_only
        worker_init_fn: NULL
      # Places365
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.places365_val_path_file_list}
          label_file_list: ${constants.places365_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: places365
        worker_init_fn: NULL
      # # Food101
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 6
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101
        worker_init_fn: NULL
      # # Pets
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets
        worker_init_fn: NULL
  model:
    zero_shot_with_text_targets:
      in1k:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list: ${constants.in1k_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list: ${constants.in1k_zs_templates_list}
      sunrgbd_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.sun_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      nyuv2scene_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.nyuv2scene_zs_classnames_list}
          templates: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.templates}
      places365:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.places365_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      food101:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      pets:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
      sunrgbd_image_only: ${.in1k}
      nyuv2scene_image_only: ${.in1k}
      places365: ${.in1k}
      food101: ${.in1k}
      pets: ${.in1k}
