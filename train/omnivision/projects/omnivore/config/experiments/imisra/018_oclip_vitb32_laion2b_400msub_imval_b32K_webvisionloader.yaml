# @package _global_

defaults:
  - /experiments/imisra/016_oclip_vitb32_laion400m_imval_b32K_webvisionloader

data_module:
  _target_: omnivision.data_module.base_data_module.BaseDataModule
  train:
    _target_: omnivore.data.webdataset_helpers.WebVisionDatasetBatchedWithLoader
    base_dataset_fn:
      _target_: omnivore.data.webdataset_helpers.get_wds_dataset_batched
      _partial_: True
      urls:
        _target_: omnivore.utils.data.FileLoader.load
        return_idx: False
        path_list:
          - /checkpoint/imisra/datasets/laion/laion2b-en_subset400m_tarlist.pkl
      dataset_size_file: /checkpoint/imisra/datasets/laion/laion2b-en_subset400m_tarlist_numfiles.npy
      batch_size: 256
      num_workers: 14
      preprocess_txt:
        _target_: slip.tokenizer.SimpleTokenizer
        bpe_path_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/bpe_simple_vocab_16e6.txt.gz
          - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
      preprocess_img:
        _target_: torchvision.transforms.Compose
        transforms:
          - _target_: torchvision.transforms.RandomResizedCrop
            size: 224
            interpolation: 3
            scale: [0.9, 1.0]
          - _target_: omnivore.data.transforms.basic.PILToRGB
          - _target_: torchvision.transforms.ToTensor
          - _target_: torchvision.transforms.Normalize
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]
    base_loader_fn:
      _target_: omnivore.data.webdataset_helpers.get_wds_loader
      num_workers: ${..base_dataset_fn.num_workers}
      collate_fn:
        _target_: omnivore.data.webdataset_helpers.BatchToSampleText
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
          input_batch_is_collated: True
      _partial_: True
