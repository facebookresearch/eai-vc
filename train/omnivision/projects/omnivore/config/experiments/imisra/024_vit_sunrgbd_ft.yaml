# @package _global_

defaults:
  - /experiments/imisra/base_nopl.yaml
  - _self_

launcher:
  gpus_per_node: 8
  num_nodes: 2

trainer:
  max_epochs: 100

  distributed:
    find_unused_parameters: True # channel dropping leads to unused patch projection params

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
        concatenate_depth_and_rgb_channels: False
        path_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_image_names.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/train_image_names.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_image_names.npy
        label_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_labels.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/train_labels.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_labels.npy
        depth_path_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_depth_names.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/train_depth_names.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_depth_names.npy
        # new_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
        # new_depth_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
        # new_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
        # new_depth_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
        # AWS
        new_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images/
        new_depth_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images_disparity/
        transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                  max_depth: 75
                  clamp_max_before_scale: True
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  interpolation: 2
                - _target_: torchvision.transforms.RandomHorizontalFlip
                - _target_: omnivore.data.transforms.image_rgbd.RandAugment3d  # Essentially autoagument rand-m9-mstd0.5-inc1
                  num_ops: 2
                  magnitude: 9
                  interpolation: 2
                - _target_: omnivore.data.transforms.image_rgbd.ColorJitter3d
                  brightness: 0.4
                  contrast: 0.4
                  saturation: 0.4
                  hue: 0.4
                - _target_: torchvision.transforms.RandomErasing
                  p: .25
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406, 0.0]
                  std: [0.229, 0.224, 0.225, 1.0]
      shuffle: True
      batch_size: 32
      num_workers: 12
      pin_memory: False
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: sunrgbd
        batch_transforms:
        - _target_: omnivore.data.transforms.cutmixup.CutMixUp
          mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
          cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
          prob: 1.0 # probability of applying mixup or cutmix per batch or element
          switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
          mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
          correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
          label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
          num_classes: 19 # number of classes for target
        - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepthBatch
        - _target_: omnivore.data.transforms.image_rgbd_sample.DropVisionDepth
          vision_depth_drop_prob: 0.5
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
        concatenate_depth_and_rgb_channels: False
        path_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_image_names.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/test_image_names.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_image_names.npy
        label_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_labels.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/test_labels.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_labels.npy
        depth_path_file_list:
          - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_depth_names.npy # AWS
          - /checkpoint/kalyanv/data/sunrgbd/test_depth_names.npy
          - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_depth_names.npy
        # new_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
        # new_depth_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
        # new_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
        # new_depth_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
        # AWS
        new_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images/
        new_depth_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images_disparity/
        transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                  max_depth: 75
                  clamp_max_before_scale: True
                - _target_: torchvision.transforms.Resize
                  size: 224
                  interpolation: 3
                - _target_: torchvision.transforms.CenterCrop
                  size: 224
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406, 0.0]
                  std: [0.229, 0.224, 0.225, 1.0]
          - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
      shuffle: True
      batch_size: 64
      num_workers: 12
      pin_memory: False
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: sunrgbd
      worker_init_fn: NULL
  model:
    _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
    postprocessors: NULL
    head_to_postprocessor: NULL
    modality_preprocessors:
      - name: "rgbdt_preprocessor"
        preprocessor:
          _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
          img_size:
          - 3
          - 224
          - 224
          num_cls_tokens: 1
          pos_embed_fn:
            _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
            _partial_: true
            learnable: true
          depth_stem:
            _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
            proj_stem:
            - _target_: torch.nn.Conv2d
              kernel_size: 16
              in_channels: 1
              out_channels: ${trainer.model.trunk.embed_dim}
              stride: ${.kernel_size}
              bias: False
            norm_layer:
              _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.trunk.embed_dim}
          rgbt_stem:
            _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
            proj_stem:
            - _target_: torch.nn.Conv2d
              kernel_size: 16
              in_channels: 3
              out_channels: ${trainer.model.trunk.embed_dim}
              stride: ${.kernel_size}
              bias: False
            norm_layer:
              _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.trunk.embed_dim}
    sample_to_modality_preprocessor:
      - sample_type: ${get_class:omnivore.data.api.BatchVisionSample}
        sample_field_to_modality:
        - input_fields: ["vision"]
          preprocessor_name: rgbdt_preprocessor
          output_key: "vision_tokens"
          output_key_for_dict: False
      - sample_type: ${get_class:omnivore.data.api.BatchDepthSample}
        sample_field_to_modality:
        - input_fields: ["depth"]
          preprocessor_name: rgbdt_preprocessor
          output_key: "vision_tokens"
          output_key_for_dict: False
      - sample_type: ${get_class:omnivore.data.api.BatchVisionDepthSample}
        sample_field_to_modality:
        - input_fields: ["vision", "depth"]
          preprocessor_name: rgbdt_preprocessor
          output_key: "vision_tokens"
          output_key_for_dict: False
    trunk:
      _target_: omnivore.models.simple_transformer.SimpleTransformer
      embed_dim: 768
      num_blocks: 12
      ffn_dropout_rate: 0.0
      drop_path_rate: 0.0 # OpenCLIP
      attn_target:
        _target_: omnivore.models.simple_transformer.MultiheadAttention
        embed_dim: ${..embed_dim}
        num_heads: 12
        dropout: 0.0
        bias: True
        add_bias_kv: True
        _partial_: True
      pre_transformer_layer:
        _target_: omnivore.models.helpers.EinOpsRearrange
        rearrange_expr: "b l d -> l b d"
      post_transformer_layer:
        _target_: omnivore.models.helpers.EinOpsRearrange
        rearrange_expr: "l b d -> b l d"
    heads:
      - head:
          _target_: torch.nn.Sequential
          _args_:
          - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
            normalized_shape: ${trainer.model.trunk.embed_dim}
          - _target_: omnivore.models.pooling_helpers.SelectElement
            index: 0 # select CLS token
          - _target_: torch.nn.Linear
            in_features: ${trainer.model.trunk.embed_dim}
            out_features: 19 # number of classes
        fork_module: ""
        preprocessed_input_key: "vision_tokens"
        output_key: "image_embed"
  optim:
    gradient_clip: NULL

    amp:
      enabled: False
      amp_dtype: float16 # bfloat16 or float16

    optimizer:
      _target_: torch.optim.AdamW
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 5e-4 # batch size of 512
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-5
            lengths: [0.0166, 0.9834]  # warm for 5 epochs
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
             - '*.bias'
             - '*.pos_embed'
             - '*.cls_token'
          module_cls_names: ['torch.nn.LayerNorm']
  meters:
    train:
      sunrgbd:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
    val:
      sunrgbd:
        accuracy_top1:
          _target_: omnivore.meters.dict_accuracy_meter.DictAccuracyMeter
          top_k: 1
          dict_key: "image_embed"
        accuracy_top5:
          _target_: omnivore.meters.dict_accuracy_meter.DictAccuracyMeter
          top_k: 5
          dict_key: "image_embed"
  loss:
    sunrgbd:
      _target_: omnivore.losses.dict_apply_loss.DictApplyLoss
      loss_fn:
        _target_: torch.nn.CrossEntropyLoss
      dict_key: "image_embed"
