# @package _global_

# Follow 027
# No masking in the text encoder

defaults:
  - /experiments/imisra/027_simpletx_vitb32_laion21M_imval_b32K_msclip_params_ep32


trainer:
  model:
    multimodal_model:
      _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
      modality_preprocessors:
        - name: "rgbdt_preprocessor"
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
            img_size:
            - 3
            - 224
            - 224
            num_cls_tokens: 1
            pos_embed_fn:
              _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
              _partial_: true
              learnable: true
            depth_stem: NULL
            rgbt_stem:
              _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
              proj_stem:
              - _target_: torch.nn.Conv2d
                kernel_size: 32
                in_channels: 3
                out_channels: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                stride: ${.kernel_size}
                bias: False
              norm_layer:
                _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
        - name: "text_preprocessor"
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.TextPreprocessor
            context_length: 77
            vocab_size: 49408
            embed_dim: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            causal_masking: False
            num_cls_tokens: 1
            supply_seq_len_to_head: False
      heads:
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0 # select CLS token
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: 512
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
          fork_module: ""
          preprocessed_input_key: "vision_tokens"
          output_key: "image_embed"
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm 
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0 # select CLS token
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: 512
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
          fork_module: ""
          preprocessed_input_key: "text_tokens"
          output_key: "text_embed"
