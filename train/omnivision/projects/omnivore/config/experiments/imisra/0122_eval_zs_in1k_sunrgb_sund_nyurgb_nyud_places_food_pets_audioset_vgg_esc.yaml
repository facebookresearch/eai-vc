# @package _global_

defaults:
  - /experiments/imisra/base_dataset_paths

constants:
  rgb_crop_size: ???
  sunrgbd_interp_int: 2

trainer:
  data:
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      # ImageNet-1K
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.in1k_val_path_file_list}
          label_file_list: ${constants.in1k_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      # SUN RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_image_only
        worker_init_fn: NULL
      # SUN RGB-D depth only
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          depth_path_file_list: ${constants.sun_test_depth_path_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          new_depth_prefix: ${constants.sun_depth_prefix}
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: NULL
                    compute_max_per_sample: True
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: ${constants.sunrgbd_interp_int}
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.sun_rgbdispmax_mean}
                    std: ${constants.sun_rgbdispmax_std}
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_depth_only
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0
      # NYUv2 RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_image_only
        worker_init_fn: NULL
      # NYUv2 RGB-D depth only
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          depth_path_file_list: ${constants.nyuv2scene_test_depth_path_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          new_depth_prefix: ${constants.nyuv2scene_depth_prefix}
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: NULL
                    compute_max_per_sample: True
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: ${constants.sunrgbd_interp_int}
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.nyuv2scene_rgbdispmax_mean}
                    std: ${constants.nyuv2scene_rgbdispmax_std}
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_depth_only
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0
      # Places365
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.places365_val_path_file_list}
          label_file_list: ${constants.places365_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: places365
        worker_init_fn: NULL
      # # Food101
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 6
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101
        worker_init_fn: NULL
      # # Pets
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets
        worker_init_fn: NULL
      # AudioSet
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          label_type: csv
          new_prefix: /datasets01/audioset/042319/data/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -4.268
                  std: ${times:4.569,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: true
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: audioset
          convert_label_to_one_hot_num_classes: 527
        worker_init_fn: null
      # AudioSet video
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.VideoPathDataset
          decode_audio: false
          label_type: csv
          remove_prefix: eval_segments/video/
          new_prefix: /fsx-omnivore/rgirdhar/data/audioset/eval_segments/video_mp4-288p/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          frame_sampler:
            _target_: pytorchvideo.transforms.UniformTemporalSubsample
            num_samples: ${constants.video_num_frames}
          decoder: decord
          normalize_to_0_1: true
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: pytorchvideo.transforms.ShortSideScale
                  size: ${constants.rgb_crop_size}
                - _target_: torchvision.transforms.CenterCrop
                  size: ${constants.rgb_crop_size}
                - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                  mean: ${constants.in1k_rgb_mean}
                  std: ${constants.in1k_rgb_std}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: true
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: audioset_video
          convert_label_to_one_hot_num_classes: 527
        worker_init_fn: null
      # ESC
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          new_prefix: /fsx-omnivore/rgirdhar/data/ESC/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:5,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -6.6268077
                  std: ${times:5.358466,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: false
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: esc_fold1
        worker_init_fn: null
      # VGGSound
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          new_prefix: /fsx-omnivore/rgirdhar/data/VGGSound/wav_24k/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -4.268
                  std: ${times:4.569,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: false
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: vggsound
        worker_init_fn: null
  model:
    zero_shot_with_text_targets:
      in1k:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list: ${constants.in1k_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list: ${constants.in1k_zs_templates_list}
      sunrgbd_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.sun_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      sunrgbd_depth_only: ${.sunrgbd_image_only}
      nyuv2scene_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.nyuv2scene_zs_classnames_list}
          templates: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.templates}
      nyuv2scene_depth_only: ${.nyuv2scene_image_only}
      places365:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.places365_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      food101:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      pets:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      audioset:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/label_names.npy
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: false
            path_list: ${constants.in1k_zs_templates_list}
      audioset_video: ${.audioset}
      esc_fold1:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/label_names.npy
          templates: ${...audioset.label_strings.templates}
      vggsound:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/label_names.npy
          templates: ${...audioset.label_strings.templates}
  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
      sunrgbd_image_only: ${.in1k}
      sunrgbd_depth_only: ${.in1k}
      nyuv2scene_image_only: ${.in1k}
      nyuv2scene_depth_only: ${.in1k}
      places365: ${.in1k}
      food101: ${.in1k}
      pets: ${.in1k}
      audioset:
        mAP:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
            multilabel_mode: recall
      audioset_video:
        mAP: ${..audioset.mAP}
        accuracy_top5: ${..audioset.accuracy_top5}
      esc_fold1: ${.in1k}
      vggsound: ${.in1k}

