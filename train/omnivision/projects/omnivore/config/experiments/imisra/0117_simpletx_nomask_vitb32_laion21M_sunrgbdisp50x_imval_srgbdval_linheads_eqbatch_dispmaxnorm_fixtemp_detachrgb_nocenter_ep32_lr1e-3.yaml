# @package _global_

defaults:
  - /experiments/imisra/085_simpletx_nomask_vitb32_laion21M_sunrgbdisp50x_imval_srgbdval_linheads_eqbatch_dispmaxnorm_detachrgb_ep32

trainer:
  model:
    multimodal_model:
      postprocessors:
          - name: "normalize"
            postprocessor:
              _target_: omnivore.models.helpers.Normalize
              dim: -1
          - name: "normalize_and_scale_text"
            postprocessor:
              _target_: torch.nn.Sequential
              _args_:
                - _target_: omnivore.models.helpers.Normalize
                  dim: -1
                - _target_: omnivore.models.helpers.LearnableLogitScaling
          - name: "normalize_and_scale_depth"
            postprocessor:
              _target_: torch.nn.Sequential
              _args_:
                - _target_: omnivore.models.helpers.Normalize
                  dim: -1
                - _target_: omnivore.models.helpers.LearnableLogitScaling
                  logit_scale_init: ${divide:1,${constants.inv_depth_temp}} # 1/0.1 as in SimCLR
                  learnable: ${constants.depth_temp_learnable}
  optim:
    amp:
      enabled: True
      amp_dtype: bfloat16 # bfloat16 or float16
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-6
    gradient_clip:
      _target_: omnivore.optim.helpers.GradientClipper
      max_norm: NULL # disabled
      norm_type: 2
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-5
            lengths:
              - ${divide:${constants.warmup_epochs},${trainer.max_epochs}}
              - ${subtract:1,${divide:${constants.warmup_epochs},${trainer.max_epochs}}}
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*.bias'
            - '*pos_embed'
            - '*cls_token'
            - "*log_logit_scale"
          module_cls_names: ["torch.nn.LayerNorm"]
constants:
  final_embed_dim: 512
  inv_depth_temp: 0.1
  depth_temp_learnable: False
