# @package _global_

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 32
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [50.0]
      datasets:
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_image_names.npy # AWS
            - /checkpoint/kalyanv/data/sunrgbd/train_image_names.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_image_names.npy
          label_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_labels.npy # AWS
            - /checkpoint/kalyanv/data/sunrgbd/train_labels.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_labels.npy
          depth_path_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_depth_names.npy # AWS
            - /checkpoint/kalyanv/data/sunrgbd/train_depth_names.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_depth_names.npy
          # new_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
          # new_depth_prefix: memcache_manifold://omnivore/tree/datasets/sunrgbd/cls_data/images/
          # new_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
          # new_depth_prefix: /checkpoint/kalyanv/data/sunrgbd/images/
          # AWS
          new_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images/
          new_depth_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images_disparity/
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: 75
                    clamp_max_before_scale: True
                  - _target_: torchvision.transforms.RandomResizedCrop
                    size: 224
                    interpolation: 2
                  - _target_: torchvision.transforms.RandomHorizontalFlip
                  - _target_: omnivore.data.transforms.image_rgbd.RandAugment3d  # Essentially autoagument rand-m9-mstd0.5-inc1
                    num_ops: 2
                    magnitude: 9
                    interpolation: 2
                  - _target_: omnivore.data.transforms.image_rgbd.ColorJitter3d
                    brightness: 0.4
                    contrast: 0.4
                    saturation: 0.4
                    hue: 0.4
                  - _target_: torchvision.transforms.RandomErasing
                    p: .25
                  - _target_: torchvision.transforms.Normalize
                    mean: [0.485, 0.456, 0.406, 0.0]
                    std: [0.229, 0.224, 0.225, 1.0]
        shuffle: True
        batch_size: 32
        num_workers: 12
        pin_memory: False
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepthBatch
        worker_init_fn: NULL
    
  model:
    _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
    modality_preprocessors:
      - name: "rgbt_preprocessor"
        preprocessor:
          _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
          img_size:
          - 3
          - 224
          - 224
          num_cls_tokens: 1
          pos_embed_fn:
            _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
            _partial_: true
            learnable: true
          depth_stem: NULL
          rgbt_stem:
            _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
            proj_stem:
            - _target_: torch.nn.Conv2d
              kernel_size: 32
              in_channels: 3
              out_channels: ${trainer.model.trunks.0.trunk.embed_dim}
              stride: ${.kernel_size}
              bias: False
            norm_layer:
              _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.trunks.0.trunk.embed_dim}
      - name: "d_preprocessor"
        preprocessor:
          _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
          img_size:
          - 1
          - 224
          - 224
          num_cls_tokens: 1
          pos_embed_fn:
            _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
            _partial_: true
            learnable: true
          rgbt_stem: NULL
          depth_stem:
            _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
            proj_stem:
            - _target_: torch.nn.Conv2d
              kernel_size: 32
              in_channels: 1
              out_channels: ${trainer.model.trunks.0.trunk.embed_dim}
              stride: ${.kernel_size}
              bias: False
            norm_layer:
              _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.trunks.0.trunk.embed_dim}
    sample_to_modality_preprocessor:
      - sample_type: ${get_class:omnivore.data.api.BatchVisionDepthSample}
        sample_field_to_modality:
        - input_fields: ["vision"]
          preprocessor_name: rgbt_preprocessor
          output_key: "vision_tokens"
          output_key_for_dict: False
        - input_fields: ["depth"]
          preprocessor_name: d_preprocessor
          output_key: "depth_tokens"
          output_key_for_dict: False
    trunks:
      - name: vision
        trunk:
          _target_: omnivore.models.simple_transformer.SimpleTransformer
          embed_dim: 768
          num_blocks: 12
          ffn_dropout_rate: 0.0
          drop_path_rate: 0.0 # OpenCLIP
          attn_target:
            _target_: omnivore.models.simple_transformer.MultiheadAttention
            embed_dim: ${..embed_dim}
            num_heads: 12
            dropout: 0.0
            bias: True
            add_bias_kv: True
            _partial_: True
          pre_transformer_layer:
            _target_: omnivore.models.helpers.EinOpsRearrange
            rearrange_expr: "b l d -> l b d"
          post_transformer_layer:
            _target_: omnivore.models.helpers.EinOpsRearrange
            rearrange_expr: "l b d -> b l d"
    tokens_to_trunks:
      - trunk_name: vision
        input_keys:
          - vision_tokens
          - depth_tokens
    heads:
      - head:
          _target_: torch.nn.Sequential
          _args_:
          - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
            normalized_shape: ${trainer.model.trunks.0.trunk.embed_dim}
          - _target_: omnivore.models.pooling_helpers.SelectElement
            index: 0 # select CLS token
          - _target_: omnivision.model.model_init_utils.init_parameters
            model:
              _target_: torch.nn.Linear
              in_features: ${trainer.model.trunks.0.trunk.embed_dim}
              out_features: 512
              bias: False # OpenCLIP
            init_fns:
              weight:
                _target_: torch.nn.init.normal_
                _partial_: True
                mean: 0
                std: 0.03608 # 768 ** -0.5
        fork_module: ""
        preprocessed_input_key: "vision_tokens"
        output_key: "image_embed"
      - head:
          _target_: torch.nn.Sequential
          _args_:
          - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
            normalized_shape: ${trainer.model.trunks.0.trunk.embed_dim}
          - _target_: omnivore.models.pooling_helpers.SelectElement
            index: 0 # select CLS token
          - _target_: omnivision.model.model_init_utils.init_parameters
            model:
              _target_: torch.nn.Linear
              in_features: ${trainer.model.trunks.0.trunk.embed_dim}
              out_features: 512
              bias: False # OpenCLIP
            init_fns:
              weight:
                _target_: torch.nn.init.normal_
                _partial_: True
                mean: 0
                std: 0.03608 # 768 ** -0.5
        fork_module: ""
        preprocessed_input_key: "depth_tokens"
        output_key: "depth_embed"
    postprocessors:
      - name: "normalize"
        postprocessor:
          _target_: omnivore.models.helpers.Normalize
          dim: -1
      - name: "normalize_and_scale_depth"
        postprocessor:
          _target_: torch.nn.Sequential
          _args_:
            - _target_: omnivore.models.helpers.Normalize
              dim: -1
            - _target_: omnivore.models.helpers.LearnableLogitScaling
    head_to_postprocessor:
      - input_key: "image_embed"
        postprocessor_name: "normalize"
      - input_key: "depth_embed"
        postprocessor_name: "normalize_and_scale_depth" # only apply logit scaling to 1 dim
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-6
    gradient_clip: NULL
    amp:
      enabled: True
      amp_dtype: float16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1.6e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1.6e-4
            lengths: [0.00625, 0.99375]  # warm for 2440 iters; 1 epoch = 305 iter
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*.bias'
            - '*pos_embed'
            - '*cls_token'
            - "*log_logit_scale"
          module_cls_names: ["torch.nn.LayerNorm"]

  # meters:
  #   val:
  #     in1k:
  #       accuracy_top1:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 1
  #       accuracy_top5:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 5
  #     sunrgbd_image_only:
  #       accuracy_top1:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 1
  #       accuracy_top5:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 5
  #     sunrgbd_depth_only:
  #       accuracy_top1:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 1
  #       accuracy_top5:
  #         _target_: omnivision.meters.accuracy_meter.AccuracyMeter
  #         top_k: 5

  loss:
    sunrgbd:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: image_embed
      feat2_name: depth_embed
      logit_scale_name: NULL
      normalize: False # SimpleTx normalizes outputs in the model

  distributed:
    backend: nccl
    comms_dtype: float16
    find_unused_parameters: False

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True

launcher:
  num_nodes: 2
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
