# @package _global_

defaults:
  - /experiments/imisra/080_simpletx_nomask_vitb32_laion21M_sun_lcrop_realtext_dispmaxnorm_b4K_lr1e-3_ep32

trainer:
  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [50.0]
      datasets:
      # SUN RGB-D Depth Only; Drop the RGB
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/classnames_zs.npy
            - /checkpoint/imisra/datasets/sunrgbd/label_files/classnames_zs.npy
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list:
              - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
          base_dataset:
            _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
            concatenate_depth_and_rgb_channels: False
            copy_on_read: True
            copy_on_read_dst_basename: ${...collate_fn.output_key}
            path_file_list:
              - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_image_names.npy # AWS
              - /checkpoint/kalyanv/data/sunrgbd/train_image_names.npy
              - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_image_names.npy
            label_file_list:
              - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_labels.npy # AWS
              - /checkpoint/kalyanv/data/sunrgbd/train_labels.npy
              - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_labels.npy
            depth_path_file_list:
              - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/train_disparity_names.npy # AWS
              - /checkpoint/imisra/datasets/sunrgbd/label_files/train_disparity_names.npy
              - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/train_disparity_names.npy
            new_prefix: ${constants.sun_rgb_prefix}
            new_depth_prefix: ${constants.sun_depth_prefix}
            transforms:
              - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                      max_depth: NULL
                      compute_max_per_sample: True
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: 224
                      interpolation: 2
                      scale: [0.75, 1.0]
                    - _target_: torchvision.transforms.RandomHorizontalFlip
                    - _target_: omnivore.data.transforms.image_rgbd.RandAugment3d  # Essentially autoagument rand-m9-mstd0.5-inc1
                      num_ops: 2
                      magnitude: 9
                      interpolation: 2
                    - _target_: omnivore.data.transforms.image_rgbd.ColorJitter3d
                      brightness: 0.4
                      contrast: 0.4
                      saturation: 0.4
                      hue: 0.4
                    - _target_: torchvision.transforms.RandomErasing
                      p: .25
                    - _target_: torchvision.transforms.Normalize
                      mean: [0.485, 0.456, 0.406, 0.480]
                      std: [0.229, 0.224, 0.225, 0.165]
              - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: True
        batch_size: ${constants.batch_size}
        num_workers: 12
        pin_memory: False
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0
          batch_kwargs:
            model_fwd_kwargs:
              use_checkpoint: ${constants.use_grad_checkpoint}
        worker_init_fn: NULL
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      # SUN RGB-D depth only
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_image_names.npy # AWS
            - /checkpoint/imisra/datasets/sunrgbd/label_files/test_image_names.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_image_names.npy
          label_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_labels.npy # AWS
            - /checkpoint/imisra/datasets/sunrgbd/label_files/test_labels.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_labels.npy
          depth_path_file_list:
            - /fsx-omnivore/imisra/datasets/sunrgbd/label_files/test_disparity_names.npy # AWS
            - /checkpoint/imisra/datasets/sunrgbd/label_files/test_disparity_names.npy
            - manifold://omnivore/tree/datasets/sunrgbd/scene_challenge/test_disparity_names.npy
          new_prefix: ${constants.sun_rgb_prefix}
          new_depth_prefix: ${constants.sun_depth_prefix}
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: NULL
                    compute_max_per_sample: True
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 2
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.Normalize
                    mean: [0.485, 0.456, 0.406, 0.480]
                    std: [0.229, 0.224, 0.225, 0.165]
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: False
        batch_size: 64
        num_workers: 12
        pin_memory: False
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_depth_only
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0

  model:
    multimodal_model:
      sample_to_modality_preprocessor:
        - sample_type: ${get_class:omnivore.data.api.BatchDepthTextSample}
          sample_field_to_modality:
          - input_fields: ["depth"]
            preprocessor_name: d_preprocessor
            output_key: "depth_tokens_vision_targets"
            output_key_for_dict: False
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens_vision_targets"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionTextSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens_vision_targets"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchTextSample}
          sample_field_to_modality:
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens_vision_targets"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionDepthSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
          - input_fields: ["depth"]
            preprocessor_name: d_preprocessor
            output_key: "depth_tokens_vision_targets"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionDepthTextSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
          - input_fields: ["depth"]
            preprocessor_name: d_preprocessor
            output_key: "depth_tokens_vision_targets"
            output_key_for_dict: False
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens_vision_targets"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchDepthSample}
          sample_field_to_modality:
          - input_fields: ["depth"]
            preprocessor_name: d_preprocessor
            output_key: "depth_tokens_vision_targets"
            output_key_for_dict: False
  loss:
    laion:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: vision_embed
      feat2_name: text_embed_vision_targets
      logit_scale_name: NULL
      normalize: False # SimpleTx normalizes outputs in the model
    sunrgbd:
      _target_: omnivore.losses.concat_loss.ConcatLoss
      loss_fns:
        - _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
          feat1_name: text_embed_vision_targets
          feat2_name: depth_embed_text_targets
          logit_scale_name: NULL
          normalize: False # SimpleTx normalizes outputs in the model
  optim:
    gradient_clip:
      _target_: omnivore.optim.helpers.GradientClipper
      max_norm: 1
      norm_type: 2
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1.6e-4
            lengths:
              - ${divide:${constants.warmup_epochs},${trainer.max_epochs}}
              - ${subtract:1,${divide:${constants.warmup_epochs},${trainer.max_epochs}}}
            interval_scaling: ['rescaled', 'rescaled']

constants:
  use_grad_checkpoint: False
