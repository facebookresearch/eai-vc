# @package _global_

defaults:
  - /experiments/imisra/0164_vitb16_shared_p-2-16-16_laion1b-25Mroll_b32K_cg5_slr1e-3_elr1e-5_maskgatherloss_ep32


trainer:
  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [1.0]
      datasets:
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.vision_text_dataset.VisionTextDataset
            base_dataset:
              _target_: omnivore.data.webdataset_helpers.WebVisionTextPipeline
              base_dataset_length: 25e6
              base_dataset_fn:
                _target_: omnivore.data.webdataset_helpers.get_wds_dataset
                _partial_: True
                resampled: True
                urls:
                  _target_: omnivore.utils.data.FileLoader.load
                  return_idx: False
                  path_list:
                    - /checkpoint/imisra/datasets/laion/laion2B-en_clean_subset1B_tarlist.pkl
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: ${constants.rgb_crop_size}
                      interpolation: 3
                      scale: [0.9, 1.0]
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.Normalize
                      mean: ${constants.in1k_rgb_mean}
                      std: ${constants.in1k_rgb_std}
              - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
                base_transform:
                  _target_: slip.tokenizer.SimpleTokenizer
                  bpe_path_list: ${constants.bpe_path_list}
          shuffle: True
          batch_size: ${constants.laion_batch_size}
          num_workers: 12
          pin_memory: True
          drop_last: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: laion
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: ${constants.use_grad_checkpoint_laion}
                checkpoint_blk_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
          worker_init_fn: NULL
  model:
    _target_: omnivore.models.multimodal_wrapper.MultiModalZeroShotWithTextTargetsWrapper
    multimodal_model:
      _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
      list_input_reduction: mean
      modality_preprocessors:
        - name: rgbt_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
            img_size:
              - 3
              - ${constants.rgb_crop_size}
              - ${constants.rgb_crop_size}
            num_cls_tokens: 1
            pos_embed_fn:
              _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
              _partial_: true
              learnable: ${constants.learnable_pos_rgbt}
            depth_stem: null
            rgbt_stem:
              _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
              proj_stem:
              - _target_: torch.nn.Conv2d
                in_channels: 3
                kernel_size: 16
                out_channels: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                stride: ${.kernel_size}
                bias: false
              norm_layer:
                _target_: torch.nn.LayerNorm
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
        - name: text_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.TextPreprocessor
            context_length: 77
            vocab_size: 49408
            embed_dim: ${trainer.model.multimodal_model.trunks.1.trunk.embed_dim}
            causal_masking: True
      trunks:
        - name: vision
          trunk:
            _target_: omnivore.models.simple_transformer.SimpleTransformer
            embed_dim: 768
            num_blocks: 12
            ffn_dropout_rate: 0.0
            drop_path_rate: 0.0 # OpenCLIP
            attn_target:
              _target_: omnivore.models.simple_transformer.MultiheadAttention
              embed_dim: ${..embed_dim}
              num_heads: 12
              dropout: 0.0
              bias: True
              add_bias_kv: True
              _partial_: True
            pre_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "b l d -> l b d"
            post_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "l b d -> b l d"
        - name: text
          trunk:
            _target_: omnivore.models.simple_transformer.SimpleTransformer
            embed_dim: 512
            num_blocks: 12
            ffn_dropout_rate: 0.0
            drop_path_rate: 0.0 # OpenCLIP
            attn_target:
              _target_: omnivore.models.simple_transformer.MultiheadAttention
              embed_dim: ${..embed_dim}
              num_heads: 8
              dropout: 0.0
              bias: True
              add_bias_kv: True
              _partial_: True
            pre_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "b l d -> l b d"
            post_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "l b d -> b l d"
      tokens_to_trunks:
        - trunk_name: vision
          input_keys:
            - vision_tokens
        - trunk_name: text
          input_keys:
            - text_tokens
      heads:
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: ${constants.head_final_embed_dim}
                bias: false
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: true
                  mean: 0
                  std: 0.03608
          fork_module: ''
          preprocessed_input_key: vision_tokens
          output_key: vision_embed
        - head:
            _target_: omnivore.models.pooling_helpers.SelectEOSAndProject
            proj:
              _target_: torch.nn.Sequential
              _args_:
              - _target_: torch.nn.LayerNorm
                normalized_shape: ${trainer.model.multimodal_model.trunks.1.trunk.embed_dim}
              - _target_: omnivision.model.model_init_utils.init_parameters
                model:
                  _target_: torch.nn.Linear
                  in_features: ${trainer.model.multimodal_model.trunks.1.trunk.embed_dim}
                  out_features: ${constants.head_final_embed_dim}
                  bias: false
                init_fns:
                  weight:
                    _target_: torch.nn.init.normal_
                    _partial_: true
                    mean: 0
                    std: 0.03608
          fork_module: ''
          preprocessed_input_key: text_tokens
          output_key: text_embed
