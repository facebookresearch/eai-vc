# @package _global_

defaults:
  - /experiments/imisra/base_dataset_paths

constants:
  rgb_crop_size: ???
  video_clip_duration: ???
  video_num_frames: ???
  audio_num_mel_bins: ???
  audio_target_len: ???
  sunrgbd_interp_int: 2

trainer:
  data:
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      # ImageNet-1K
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.in1k_val_path_file_list}
          label_file_list: ${constants.in1k_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      # SUN RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_image_only
        worker_init_fn: NULL
      # SUN RGB-D depth only
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list: ${constants.sun_test_path_file_list}
          label_file_list: ${constants.sun_test_label_file_list}
          depth_path_file_list: ${constants.sun_test_depth_path_file_list}
          new_prefix: ${constants.sun_rgb_prefix}
          new_depth_prefix: ${constants.sun_depth_prefix}
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: NULL
                    compute_max_per_sample: True
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: ${constants.sunrgbd_interp_int}
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.sun_rgbdispmax_mean}
                    std: ${constants.sun_rgbdispmax_std}
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: sunrgbd_depth_only
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0
      # NYUv2 RGB-D Image
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_image_only
        worker_init_fn: NULL
      # NYUv2 RGB-D depth only
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImageWithDepthPathDataset
          concatenate_depth_and_rgb_channels: False
          path_file_list: ${constants.nyuv2scene_test_path_file_list}
          label_file_list: ${constants.nyuv2scene_test_label_file_list}
          depth_path_file_list: ${constants.nyuv2scene_test_depth_path_file_list}
          new_prefix: ${constants.nyuv2scene_rgb_prefix}
          new_depth_prefix: ${constants.nyuv2scene_depth_prefix}
          transforms:
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: omnivore.data.transforms.image_rgbd.DepthNorm
                    max_depth: NULL
                    compute_max_per_sample: True
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: ${constants.sunrgbd_interp_int}
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.nyuv2scene_rgbdispmax_mean}
                    std: ${constants.nyuv2scene_rgbdispmax_std}
            - _target_: omnivore.data.transforms.image_rgbd_sample.VisionDepthConcatChannelToVisionDepth
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: nyuv2scene_depth_only
          batch_transforms:
          - _target_: omnivore.data.transforms.image_rgbd_sample.DropVision
            vision_drop_prob: 1.0
      # Places365
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.places365_val_path_file_list}
          label_file_list: ${constants.places365_val_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 64
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: places365
        worker_init_fn: NULL
      # # Food101
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.food101_test_path_file_list}
          label_file_list: ${constants.food101_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 32
        num_workers: 6
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: food101
        worker_init_fn: NULL
      # # Pets
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          copy_on_read: True
          copy_on_read_dst_basename: ${..collate_fn.output_key}
          path_file_list: ${constants.pets_test_path_file_list}
          label_file_list: ${constants.pets_test_label_file_list}
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
        shuffle: False
        batch_size: 8 # keep this small so we don't drop the batch when evaluating with a large overall batch size
        num_workers: 4
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: pets
        worker_init_fn: NULL
      # AudioSet
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          label_type: csv
          new_prefix: /datasets01/audioset/042319/data/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -4.268
                  std: ${times:4.569,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: true
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: audioset
          convert_label_to_one_hot_num_classes: 527
        worker_init_fn: null
      # AudioSet video
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.VideoPathDataset
          decode_audio: false
          label_type: csv
          remove_prefix: eval_segments/video/
          new_prefix: /fsx-omnivore/rgirdhar/data/audioset/eval_segments/video_mp4-288p/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          frame_sampler:
            _target_: pytorchvideo.transforms.UniformTemporalSubsample
            num_samples: ${constants.video_num_frames}
          decoder: decord
          normalize_to_0_1: true
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: pytorchvideo.transforms.ShortSideScale
                  size: ${constants.rgb_crop_size}
                - _target_: torchvision.transforms.CenterCrop
                  size: ${constants.rgb_crop_size}
                - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                  mean: ${constants.in1k_rgb_mean}
                  std: ${constants.in1k_rgb_std}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: true
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: audioset_video
          convert_label_to_one_hot_num_classes: 527
        worker_init_fn: null
      # ESC
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          new_prefix: /fsx-omnivore/rgirdhar/data/ESC/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:5,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -6.6268077
                  std: ${times:5.358466,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: false
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: esc_fold1
        worker_init_fn: null
      # VGGSound
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.AudioPathDataset
          num_mel_bins: ${constants.audio_num_mel_bins}
          target_length: ${constants.audio_target_len}
          new_prefix: /fsx-omnivore/rgirdhar/data/VGGSound/wav_24k/
          path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_filelist.npy
          label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_labels.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: omnivore.data.transforms.transform_wrappers.ListTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: torchvision.transforms.Normalize
                  mean: -4.268
                  std: ${times:4.569,2}
        shuffle: false
        batch_size: 32
        num_workers: 8
        pin_memory: true
        drop_last: false
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: vggsound
        worker_init_fn: null
      # Kinetics 400 (K400)
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.VideoPathDataset
          path_file_list:
            - /checkpoint/aelnouby/datasets/k400/vidpaths_val.npy
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/vidpaths_val.npy
            - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
          label_file_list:
            - /checkpoint/aelnouby/datasets/k400/labels_val.npy
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/labels_val.npy
            - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: ${constants.video_clip_duration}
            clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
          frame_sampler:
            _target_: pytorchvideo.transforms.UniformTemporalSubsample
            num_samples: ${constants.video_num_frames}
          decoder: pyav
          normalize_to_0_1: True
          transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                    - _target_: pytorchvideo.transforms.ShortSideScale
                      size: ${constants.rgb_crop_size}
                    - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                      mean: ${constants.in1k_rgb_mean}
                      std: ${constants.in1k_rgb_std}
              # Have to do this transform separately since SpatialCrop was written to
              # expect a list as input (hence can't wrap with a ListTransform)
              # TODO: Write a simpler version of spatial transform without expecting
              # lists and then just flatten the list of lists.
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                  crop_size: ${constants.rgb_crop_size}
                  num_crops: 3
        # FIX shuffle false getting ignored
        shuffle: False
        batch_size: 4
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          batch_kwargs:
            model_fwd_kwargs:
              use_checkpoint: False
          output_key: k400
        worker_init_fn: NULL
      # AudioCaps
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
          base_dataset:
            _target_: omnivore.data.path_dataset.AudioPathDataset
            num_mel_bins: ${constants.audio_num_mel_bins}
            target_length: ${constants.audio_target_len}
            new_prefix: /datasets01/audioset/042319/data/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_filelist.npy
            label_file_list: NULL
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${constants.video_clip_duration}
              clips_per_video: ${ceil_int:${divide:10,${constants.video_clip_duration}}}
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchvision.transforms.Normalize
                        # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                        mean: -4.268
                        std: ${times:4.569,2}
          captions_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_captions.npy
          caption2data_mapping_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_captions2audio.npy
          tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
        shuffle: False
        batch_size: 32
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: audiocaps
        worker_init_fn: NULL
      # Clotho
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
          base_dataset:
            _target_: omnivore.data.path_dataset.AudioPathDataset
            num_mel_bins: ${constants.audio_num_mel_bins}
            target_length: ${constants.audio_target_len}
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_filelist.npy
            label_file_list: NULL
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${constants.video_clip_duration}
              # 15-30s long audio clips, so need enough clips to cover full audio
              clips_per_video: ${ceil_int:${divide:30,${constants.video_clip_duration}}}
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchvision.transforms.Normalize
                        mean: -4.268
                        std: ${times:4.569,2}
          captions_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_captions.npy
          caption2data_mapping_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_captions2audio.npy
          tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
        shuffle: False
        batch_size: 32
        num_workers: 8
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: clotho
        worker_init_fn: NULL
      # MSR VTT
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
          base_dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            decode_audio: False
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/filelist.npy
            label_file_list: NULL
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${constants.video_clip_duration}
              # 30s is the max length of MSR-VTT videos
              clips_per_video: ${ceil_int:${divide:30,${constants.video_clip_duration}}}
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: ${constants.video_num_frames}
            decoder: decord  # since this allows for audio decoding
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                    - _target_: pytorchvideo.transforms.ShortSideScale
                      size: ${constants.rgb_crop_size}  # 256
                    - _target_: torchvision.transforms.CenterCrop
                      size: ${constants.rgb_crop_size}
                    - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                      mean: ${constants.in1k_rgb_mean}
                      std: ${constants.in1k_rgb_std}
              # Have to do this transform separately since SpatialCrop was written to
              # expect a list as input (hence can't wrap with a ListTransform)
              # TODO: Write a simpler version of spatial transform without expecting
              # lists and then just flatten the list of lists.
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                  crop_size: ${constants.rgb_crop_size}
                  num_crops: 3
          captions_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/captions.npy
          caption2data_mapping_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/captions2audio.npy
          tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
        shuffle: False
        batch_size: 100
        num_workers: 10
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: msrvtt
        worker_init_fn: NULL
      # MSCOCO
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
          base_dataset:
            _target_: omnivore.data.path_dataset.ImagePathDataset
            path_file_list:
              - /checkpoint/aelnouby/datasets/mscoco/filelist.npy
            label_file_list: NULL
            transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: ${constants.rgb_crop_size}
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: ${constants.rgb_crop_size}
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: ${constants.in1k_rgb_mean}
                    std: ${constants.in1k_rgb_std}
          captions_file_list:
            - /checkpoint/aelnouby/datasets/mscoco/captions.npy
          caption2data_mapping_file_list:
            - /checkpoint/aelnouby/datasets/mscoco/text2img_mapping.npy
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
        shuffle: False
        batch_size: 16
        num_workers: 8 # reduce workers to prevent OOMs
        pin_memory: True
        drop_last: False
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: mscoco
        worker_init_fn: NULL
  model:
    multimodal_model:
      dataset_specific_list_input_reduction:
        # Keep audiocaps output (used in val) as is for retrieval eval
        # Very important, else it will eval on avg pooled features
        audiocaps:
          - field_name: text_embed
            reduction_op: no_op
        clotho: ${.audiocaps}
        audioset:
          - field_name: audio_embed
            reduction_op: mean
        esc_fold1: ${.audioset}
        audioset_video:
          - field_name: video_embed
            reduction_op: mean
        k400: ${.audioset_video}
        msrvtt: ${.audiocaps}
        mscoco:
          - field_name: text_embed
            reduction_op: no_op
          - field_name: vision_embed
            reduction_op: no_op
    zero_shot_with_text_targets:
      in1k:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list: ${constants.in1k_zs_classnames_list}
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list: ${constants.in1k_zs_templates_list}
      sunrgbd_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.sun_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      sunrgbd_depth_only: ${.sunrgbd_image_only}
      nyuv2scene_image_only:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.nyuv2scene_zs_classnames_list}
          templates: ${trainer.model.zero_shot_with_text_targets.in1k.label_strings.templates}
      nyuv2scene_depth_only: ${.nyuv2scene_image_only}
      places365:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.places365_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      food101:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.food101_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      pets:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...in1k.label_strings.tokenizer}
          label_names_file_list: ${constants.pets_zs_classnames_list}
          templates: ${...in1k.label_strings.templates}
      audioset:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list: ${constants.bpe_path_list}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/label_names.npy
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: false
            path_list: ${constants.in1k_zs_templates_list}
      audioset_video: ${.audioset}
      esc_fold1:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/label_names.npy
          templates: ${...audioset.label_strings.templates}
      vggsound:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/label_names.npy
          templates: ${...audioset.label_strings.templates}
      k400:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
            - /checkpoint/aelnouby/datasets/k400_label_names.npy   # FAIR / AWS 
            - manifold://omnivore/tree/datasets/kinetics_400_meta/label_names.npy
          templates: ${...audioset.label_strings.templates}
  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
      sunrgbd_image_only: ${.in1k}
      sunrgbd_depth_only: ${.in1k}
      nyuv2scene_image_only: ${.in1k}
      nyuv2scene_depth_only: ${.in1k}
      places365: ${.in1k}
      food101: ${.in1k}
      pets: ${.in1k}
      audioset:
        mAP:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
            multilabel_mode: recall
      audioset_video:
        mAP: ${..audioset.mAP}
        accuracy_top5: ${..audioset.accuracy_top5}
      esc_fold1: ${.in1k}
      vggsound: ${.in1k}
      audiocaps:
        recall_text2vid:
          _target_: omnivore.meters.cross_modality_retrieval.CrossModalityRetrieval
          query_feature: text_embed
          corpus_feature: audio_embed
          topks: [1, 10]
      clotho: ${.audiocaps}
      k400: ${.in1k}
      msrvtt:
        recall_text2vid:
          _target_: omnivore.meters.cross_modality_retrieval.CrossModalityRetrieval
          query_feature: text_embed
          corpus_feature: vision_embed
          topks: [1, 5, 10]
      mscoco: ${.msrvtt}
