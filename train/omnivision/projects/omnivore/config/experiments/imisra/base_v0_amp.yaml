# @package _global_

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: ???
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  optim:
    amp:
      enabled: True
      amp_dtype: float16 # bfloat16 or float16
    gradient_clip: NULL
    optimizer: ???
    options:
      weight_decay: ???
      lr: ???

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  distributed:
   backend: nccl
   comms_dtype: NULL
   find_unused_parameters: False

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  limit_train_batches: NULL
  limit_val_batches: NULL
  env_variables: NULL

  cuda:
   # https://pytorch.org/docs/stable/backends.html
   allow_tf32: True
   cudnn_deterministic: False
   cudnn_benchmark: True

  data: ???
  model: ???

  meters: NULL
  loss: NULL

launcher:
  gpus_per_node: ???
  num_nodes: ???
  experiment_log_dir: ???

constants:
  warmup_epochs: 0.2
  sun_rgb_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images/
  sun_depth_prefix: /fsx-omnivore/imisra/datasets/sunrgbd/images_disparity/

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: "omnivision_omnivore"
  timeout_hour: 72
  cpus_per_task: 10
  partition: "omnivore"
  constraints: "" #"volta32gb"
  use_cluster: True
  port_range: [10000, 65000]
