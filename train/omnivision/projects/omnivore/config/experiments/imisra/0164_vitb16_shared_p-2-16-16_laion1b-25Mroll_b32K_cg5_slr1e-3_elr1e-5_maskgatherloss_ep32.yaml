# @package _global_

defaults:
  - /experiments/imisra/base_v0_ampbf16
  - /experiments/imisra/base_dataset_paths
  - /experiments/imisra/0122_eval_zs_in1k_sunrgb_nyurgb_places_food_pets


constants:
  rgb_crop_size: 224
  video_num_frames: 2
  head_final_embed_dim: 512
  learnable_pos_rgbt: true
  rgb_kernel_size: [2, 16, 16]
  warmup_epochs: 0.8 # 1ep over 20M
  use_grad_checkpoint_laion: true
  text_temp_learnable: True
  inv_text_temp: 0.07
  laion_batch_size: 1024
  loss_mask_with_data_valid: True
  grad_clip_norm: 5.0
  loss_all_gather_fn:
    _target_: omnivore.utils.distributed.all_gather_batch_with_grad
    _partial_: True

trainer:
  val_epoch_freq: 2
  max_epochs: 32
  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [1.0]
      datasets:
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.vision_text_dataset.VisionTextDataset
            base_dataset:
              _target_: omnivore.data.webdataset_helpers.WebVisionTextPipeline
              base_dataset_length: 25e6
              base_dataset_fn:
                _target_: omnivore.data.webdataset_helpers.get_wds_dataset
                _partial_: True
                resampled: True
                urls:
                  _target_: omnivore.utils.data.FileLoader.load
                  return_idx: False
                  path_list:
                    - /checkpoint/imisra/datasets/laion/laion2B-en_clean_subset1B_tarlist.pkl
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: ${constants.rgb_crop_size}
                      interpolation: 3
                      scale: [0.9, 1.0]
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.Normalize
                      mean: ${constants.in1k_rgb_mean}
                      std: ${constants.in1k_rgb_std}
              - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
                base_transform:
                  _target_: slip.tokenizer.SimpleTokenizer
                  bpe_path_list: ${constants.bpe_path_list}
          shuffle: True
          batch_size: ${constants.laion_batch_size}
          num_workers: 12
          pin_memory: True
          drop_last: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: laion
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: ${constants.use_grad_checkpoint_laion}
                checkpoint_blk_ids: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
          worker_init_fn: NULL
  model:
    _target_: omnivore.models.multimodal_wrapper.MultiModalZeroShotWithTextTargetsWrapper
    multimodal_model:
      _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
      list_input_reduction: mean
      modality_preprocessors:
        - name: rgbt_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
            img_size:
              - 3
              - ${constants.video_num_frames}
              - ${constants.rgb_crop_size}
              - ${constants.rgb_crop_size}
            num_cls_tokens: 1
            pos_embed_fn:
              _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
              _partial_: true
              learnable: ${constants.learnable_pos_rgbt}
            depth_stem: null
            rgbt_stem:
              _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
              proj_stem:
              - _target_: omnivore.models.PadIm2Video
                pad_type: repeat
                ntimes: 2
              - _target_: torch.nn.Conv3d
                in_channels: 3
                kernel_size: ${constants.rgb_kernel_size}
                out_channels: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                stride: ${.kernel_size}
                bias: false
              norm_layer:
                _target_: torch.nn.LayerNorm
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
        - name: text_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.TextPreprocessor
            context_length: 77
            vocab_size: 49408
            embed_dim: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            causal_masking: False
      sample_to_modality_preprocessor:
        - sample_type: ${get_class:omnivore.data.api.BatchVisionTextSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: vision_tokens
            output_key_for_dict: false
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: text_tokens
            output_key_for_dict: false
        - sample_type: ${get_class:omnivore.data.api.BatchTextSample}
          sample_field_to_modality:
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: text_tokens
            output_key_for_dict: false
        - sample_type: ${get_class:omnivore.data.api.BatchVisionSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbt_preprocessor
            output_key: vision_tokens
            output_key_for_dict: false
      trunks:
        - name: all
          trunk:
            _target_: omnivore.models.simple_transformer.SimpleTransformer
            embed_dim: 768
            num_blocks: 12
            ffn_dropout_rate: 0.0
            drop_path_rate: 0.0
            attn_target:
              _target_: omnivore.models.simple_transformer.MultiheadAttention
              embed_dim: ${..embed_dim}
              num_heads: 12
              dropout: 0.0
              bias: true
              add_bias_kv: true
              _partial_: true
            pre_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: b l d -> l b d
            post_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: l b d -> b l d
      tokens_to_trunks:
        - trunk_name: all
          input_keys:
          - vision_tokens
          - text_tokens
      heads:
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: ${constants.head_final_embed_dim}
                bias: false
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: true
                  mean: 0
                  std: 0.03608
          fork_module: ''
          preprocessed_input_key: vision_tokens
          output_key: vision_embed
        - head:
            _target_: omnivore.models.pooling_helpers.SelectEOSAndProject
            proj:
              _target_: torch.nn.Sequential
              _args_:
              - _target_: torch.nn.LayerNorm
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
              - _target_: omnivision.model.model_init_utils.init_parameters
                model:
                  _target_: torch.nn.Linear
                  in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                  out_features: ${constants.head_final_embed_dim}
                  bias: false
                init_fns:
                  weight:
                    _target_: torch.nn.init.normal_
                    _partial_: true
                    mean: 0
                    std: 0.03608
          fork_module: ''
          preprocessed_input_key: text_tokens
          output_key: text_embed
      postprocessors:
      - name: normalize
        postprocessor:
          _target_: omnivore.models.helpers.Normalize
          dim: -1
      - name: "normalize_and_scale_text"
        postprocessor:
          _target_: torch.nn.Sequential
          _args_:
            - _target_: omnivore.models.helpers.Normalize
              dim: -1
            - _target_: omnivore.models.helpers.LearnableLogitScaling
              logit_scale_init: ${divide:1,${constants.inv_text_temp}}
              learnable: ${constants.text_temp_learnable}
      head_to_postprocessor:
      - input_key: vision_embed
        postprocessor_name: normalize
      - input_key: text_embed
        postprocessor_name: normalize_and_scale_text
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
      - 0.9
      - 0.98
      eps: 1.0e-06
    gradient_clip:
      _target_: omnivore.optim.helpers.GradientClipper
      max_norm: ${constants.grad_clip_norm}
      norm_type: 2
    options:
      lr:
      - scheduler:
          _target_: fvcore.common.param_scheduler.CompositeParamScheduler
          schedulers:
          - _target_: fvcore.common.param_scheduler.LinearParamScheduler
            start_value: 1e-06
            end_value: 1e-3
          - _target_: fvcore.common.param_scheduler.CosineParamScheduler
            start_value: ${..0.end_value}
            end_value: 1e-5
          lengths:
          - ${divide:${constants.warmup_epochs},${trainer.max_epochs}}
          - ${subtract:1,${divide:${constants.warmup_epochs},${trainer.max_epochs}}}
          interval_scaling:
          - rescaled
          - rescaled
      weight_decay:
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.05
      - scheduler:
          _target_: fvcore.common.param_scheduler.ConstantParamScheduler
          value: 0.0
        param_names:
        - '*.bias'
        - '*pos_embed'
        - '*cls_token'
        module_cls_names: ["torch.nn.LayerNorm"]
  loss:
    laion:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: vision_embed
      feat2_name: text_embed
      logit_scale_name: null
      normalize: false
      mask_with_data_valid: ${constants.loss_mask_with_data_valid}
      all_gather_fn: ${constants.loss_all_gather_fn}
  distributed:
    comms_dtype: bfloat16
    find_unused_parameters: false
  
  checkpoint:
    save_freq: 2

launcher:
  num_nodes: 4
  gpus_per_node: 8
