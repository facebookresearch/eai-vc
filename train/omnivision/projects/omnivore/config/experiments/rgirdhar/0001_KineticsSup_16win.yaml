# @package _global_

data_module:
  _target_: omnivision.data_module.base_data_module.BaseDataModule
  train:
    dataset:
      _target_: omnivore.data.path_dataset.VideoPathDataset
      path_file_list:
        - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_train.npy
      label_file_list:
        - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_train.npy
      clip_sampler:
        _target_: pytorchvideo.data.clip_sampling.RandomClipSampler
        clip_duration: 2
      frame_sampler:
        _target_: pytorchvideo.transforms.UniformTemporalSubsample
        num_samples: 32
      decoder: pyav
      normalize_to_0_1: True
      transforms:
        - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
          base_transform:
            _target_: torchvision.transforms.Compose
            transforms:
            - _target_: pytorchvideo.transforms.ShortSideScale
              size: 256
            - _target_: torchvision.transforms.RandomResizedCrop
              size: 224
            - _target_: torchvision.transforms.RandomHorizontalFlip
              p: 0.5
            - _target_: torchvision.transforms._transforms_video.NormalizeVideo
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
    shuffle: True
    batch_size: 1
    num_workers: 3
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: omnivore.data.api.DefaultOmnivoreCollator
    #   _target_: omnivore.data.collators.cutmixup_collator.CutMixUpCollator
    #   mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
    #   cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
    #   prob: 1.0 # probability of applying mixup or cutmix per batch or element
    #   switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
    #   mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
    #   correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
    #   label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
    #   num_classes: 400 # number of classes for target
    # worker_init_fn: NULL
  val:
    dataset:
      _target_: omnivore.data.path_dataset.VideoPathDataset
      path_file_list: manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
      label_file_list: manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
      clip_sampler:
        _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
        clip_duration: 10
        clips_per_video: 1
      frame_sampler:
        _target_: pytorchvideo.transforms.UniformTemporalSubsample
        num_samples: 160
      decoder: pyav
      normalize_to_0_1: True
      transforms:
        - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
          base_transform:
            _target_: torchvision.transforms.Compose
            transforms:
            - _target_: pytorchvideo.transforms.ShortSideScale
              size: 224
            - _target_: torchvision.transforms._transforms_video.NormalizeVideo
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
            - _target_: omnivore.data.transforms.pytorchvideo.TemporalCrop
              frames_per_clip: 32
              stride: 40
            - _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
              crop_size: 224
              num_crops: 3
    shuffle: False
    batch_size: 1
    num_workers: 3
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: omnivore.data.api.DefaultOmnivoreCollator
    worker_init_fn: NULL

lightning_module:
  _target_: omnivore.lightning_module.omnivore_lightning_module.OmnivoreLightningModule
  model:
    _target_: torch.nn.Sequential
    _args_:
      - _target_: omnivore.models.swin_transformer.SwinTransformer3D
        pretrained: NULL
        pretrained2d: False
        patch_size: [2, 4, 4]
        embed_dim: 128
        depths: [2, 2, 18, 2]
        num_heads: [4, 8, 16, 32]
        window_size: [16, 7, 7]
        mlp_ratio: 4.
        qkv_bias: True
        qk_scale: NULL
        drop_rate: 0.
        attn_drop_rate: 0.
        drop_path_rate: 0.3
        patch_norm: True
      - _target_: torch.nn.Dropout
        p: 0.5
      - _target_: vissl.models.heads.make_conv_or_linear
        layer:
          _target_: torch.nn.Linear
          in_features: ${times:${...0.embed_dim},8}
          out_features: 400
        init_weight:
          _target_: torch.nn.init.normal_
          mean: 0
          std: 0.01
        init_bias:
          _target_: torch.nn.init.zeros_
        _recursive_: False
  optim:
    optimizer:
      _target_: torch.optim.SGD
      lr: 0.1
      weight_decay: 1e-4
      momentum: 0.9
      nesterov: True
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 8e-6
                end_value: 8e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: 8e-3
                end_value: 8e-6
            lengths: [0.3, 0.7]
            interval_scaling: ['fixed', 'fixed']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.4
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names: ['*.bias']
  meters:
    train:
      accuracy_top1:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 1
      accuracy_top5:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 5
    val:
      accuracy_top1:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 1
      accuracy_top5:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 5
  loss:
    _target_: torch.nn.CrossEntropyLoss

callbacks:
  precise_bn:
    num_batches: 2

lightning_trainer:
  _target_: pytorch_lightning.Trainer
  num_nodes: ${trainer.num_nodes}
  gpus: ${trainer.gpus_per_node}
  sync_batchnorm: False
  replace_sampler_ddp: False
  max_epochs: 30
  accelerator: gpu
  strategy: ddp

trainer:
  num_nodes: 8
  gpus_per_node: 8
  mode: train

logger:
  _target_: pytorch_lightning.loggers.TensorBoardLogger
  save_dir: ???
  name: default
  version: null

submitit_conf:
  log_save_dir: null
  name: "ptv_trainer_job"
  time: "72:00:00"
  cpus_per_task: 10
  partition: "learnlab"
  mem: "470GB"
  constraint: "volta32gb"
  mode: "prod"
  use_cluster: False

hydra:
  output_subdir: NULL

# test:
#   _target_: functools.partial
#   _args_:
#     - ${get_method:math.gcd}
#     - 12

test:
  _target_: functools.partial
  _args_:
    - ${get_method:vissl.data.collators.cutmixup_collator.cutmixup_collator}
  mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
  cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
  prob: 1.0 # probability of applying mixup or cutmix per batch or element
  switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
  mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
  correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
  label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
  num_classes: 1000 # number of classes for target
