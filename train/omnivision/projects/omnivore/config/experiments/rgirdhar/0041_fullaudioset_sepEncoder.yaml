# @package _global_

# Follow config/experiments/imisra/014_oclip_vitb32_laion400mact_imval_b32K_lr5e-4_adampt98_ep32_wdpt2_nocoloraug.yaml
# Use AMP for optimization
# Use FP16 gradient comm
# Use Omnivision Trainer


audio_num_mel_bins: 128
video_clip_duration: 10
# The 102.4 (could have been 100) is for historical reasons where we used 1024
# target length for 10s clip as per AST
audio_target_len: ${int:${times:${video_clip_duration},102.4}}
video_num_frames: ${video_clip_duration}
embed_dim: 512
learnable_pos_rgbdt: True
learnable_pos_audio: True
rgb_kernel_size: [2, 16, 16]
warmup_epochs: 0.2

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 32
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [1.0, 1.0]
      datasets:
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.vision_text_dataset.VisionTextDataset
            base_dataset:
              _target_: omnivore.data.webdataset_helpers.WebVisionTextPipeline
              base_dataset_length: 0  # no LAION data for now
              base_dataset_fn:
                _target_: omnivore.data.webdataset_helpers.get_wds_dataset
                _partial_: True
                resampled: True # needed for multi-node training
                urls:
                  _target_: omnivore.utils.data.FileLoader.load
                  return_idx: False
                  path_list:
                    - /checkpoint/imisra/datasets/laion/laion400m_subset21M_tarlist.pkl
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: 224
                      interpolation: 3
                      scale: [0.9, 1.0]
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.Normalize
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
              - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
                base_transform:
                  _target_: slip.tokenizer.SimpleTokenizer
                  bpe_path_list:
                    - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/bpe_simple_vocab_16e6.txt.gz
                    - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
          shuffle: True
          batch_size: 128
          num_workers: 12
          pin_memory: True
          drop_last: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: laion
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            decode_audio: True
            audio_num_mel_bins: ${audio_num_mel_bins}
            audio_target_len: ${audio_target_len}
            label_type: csv
            decoder_kwargs:
              sample_rate: 16000
            copy_on_read: True
            remove_prefix: unbalanced_train_segments/video/
            new_prefix: /fsx-omnivore/rgirdhar/data/audioset/unbalanced_train_segments/video_mp4-288p/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/unbalanced_train_segments_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/unbalanced_train_segments_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.RandomClipSampler
              clip_duration: ${video_clip_duration}
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: ${video_num_frames}
            decoder: decord  # since this allows for audio decoding
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                    - _target_: pytorchvideo.transforms.ShortSideScale
                      size: 224  # 256
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: 224
                    - _target_: torchvision.transforms.RandomHorizontalFlip
                      p: 0.5
                    - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchaudio.transforms.FrequencyMasking
                        freq_mask_param: 0
                      - _target_: torchaudio.transforms.TimeMasking
                        time_mask_param: 0
                      - _target_: torchvision.transforms.Normalize
                        # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                        mean: -4.268
                        std: ${times:4.569,2}
          shuffle: True
          batch_size: 120  # 128 was on cusp of OOM sometimes
          num_workers: 12
          pin_memory: True
          drop_last: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: audioset
            convert_label_to_one_hot_num_classes: 527
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: True
          worker_init_fn: NULL
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.ImagePathDataset
            path_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
            label_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.Resize
                      size: 224
                      interpolation: 3
                    - _target_: torchvision.transforms.CenterCrop
                      size: 224
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.Normalize
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
          shuffle: False
          batch_size: 64
          num_workers: 10
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: in1k
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.AudioPathDataset
            num_mel_bins: ${audio_num_mel_bins}
            target_length: ${audio_target_len}
            label_type: csv
            new_prefix: /datasets01/audioset/042319/data/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${video_clip_duration}
              clips_per_video: ${ceil_int:${divide:10,${video_clip_duration}}}
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchvision.transforms.Normalize
                        # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                        mean: -4.268
                        std: ${times:4.569,2}
          shuffle: False
          batch_size: 32
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: audioset
            convert_label_to_one_hot_num_classes: 527
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            decode_audio: False
            label_type: csv
            remove_prefix: eval_segments/video/
            new_prefix: /fsx-omnivore/rgirdhar/data/audioset/eval_segments/video_mp4-288p/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${video_clip_duration}
              clips_per_video: ${ceil_int:${divide:10,${video_clip_duration}}}
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: ${video_num_frames}
            decoder: decord  # since this allows for audio decoding
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                    - _target_: pytorchvideo.transforms.ShortSideScale
                      size: 224  # 256
                    - _target_: torchvision.transforms.CenterCrop
                      size: 224
                    - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
          shuffle: False
          batch_size: 120  # 128 was on cusp of OOM sometimes
          num_workers: 10
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: audioset_video
            convert_label_to_one_hot_num_classes: 527
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: True
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.AudioPathDataset
            num_mel_bins: ${audio_num_mel_bins}
            target_length: ${audio_target_len}  # Can't do 512, as position encoding needs to be handled
            new_prefix: /fsx-omnivore/rgirdhar/data/ESC/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/fold1_eval_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${video_clip_duration}
              clips_per_video: ${ceil_int:${divide:5,${video_clip_duration}}}
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchvision.transforms.Normalize
                        # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                        mean: -6.6268077
                        std: ${times:5.358466,2}
          shuffle: False
          batch_size: 32
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: esc_fold1
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.AudioPathDataset
            num_mel_bins: ${audio_num_mel_bins}
            target_length: ${audio_target_len}  # Can't do 512, as position encoding needs to be handled
            new_prefix: /fsx-omnivore/rgirdhar/data/VGGSound/wav_24k/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/eval_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${video_clip_duration}
              clips_per_video: ${ceil_int:${divide:10,${video_clip_duration}}}
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                field: audio
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                      - _target_: torchvision.transforms.Normalize
                        # Using the same as audioset since both youtube datasets.. might not be ideal though!
                        # TODO recompute this
                        mean: -4.268
                        std: ${times:4.569,2}
          shuffle: False
          batch_size: 32
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: vggsound
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
            base_dataset:
              _target_: omnivore.data.path_dataset.AudioPathDataset
              num_mel_bins: ${audio_num_mel_bins}
              target_length: ${audio_target_len}
              new_prefix: /datasets01/audioset/042319/data/
              path_file_list:
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_filelist.npy
              label_file_list: NULL
              clip_sampler:
                _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
                clip_duration: ${video_clip_duration}
                clips_per_video: ${ceil_int:${divide:10,${video_clip_duration}}}
              transforms:
                - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                  field: audio
                  base_transform:
                    _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                    base_transform:
                      _target_: torchvision.transforms.Compose
                      transforms:
                        - _target_: torchvision.transforms.Normalize
                          # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                          mean: -4.268
                          std: ${times:4.569,2}
            captions_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_captions.npy
            caption2data_mapping_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioCapsRetrieval/test_captions2audio.npy
            tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
          shuffle: False
          batch_size: 32
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: audiocaps
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
            base_dataset:
              _target_: omnivore.data.path_dataset.AudioPathDataset
              num_mel_bins: ${audio_num_mel_bins}
              target_length: ${audio_target_len}
              path_file_list:
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_filelist.npy
              label_file_list: NULL
              clip_sampler:
                _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
                clip_duration: ${video_clip_duration}
                # 15-30s long audio clips, so need enough clips to cover full audio
                clips_per_video: ${ceil_int:${divide:30,${video_clip_duration}}}
              transforms:
                - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
                  field: audio
                  base_transform:
                    _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                    base_transform:
                      _target_: torchvision.transforms.Compose
                      transforms:
                        - _target_: torchvision.transforms.Normalize
                          mean: -4.268
                          std: ${times:4.569,2}
            captions_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_captions.npy
            caption2data_mapping_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Clotho/evaluation_captions2audio.npy
            tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
          shuffle: False
          batch_size: 32
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: clotho
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.PathDatasetWithCaptions
            base_dataset:
              _target_: omnivore.data.path_dataset.VideoPathDataset
              decode_audio: False
              path_file_list:
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/filelist.npy
              label_file_list: NULL
              clip_sampler:
                _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
                clip_duration: ${video_clip_duration}
                # 30s is the max length of MSR-VTT videos
                clips_per_video: ${ceil_int:${divide:30,${video_clip_duration}}}
              frame_sampler:
                _target_: pytorchvideo.transforms.UniformTemporalSubsample
                num_samples: ${video_num_frames}
              decoder: decord  # since this allows for audio decoding
              normalize_to_0_1: True
              transforms:
                - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                  base_transform:
                    _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                    base_transform:
                      _target_: torchvision.transforms.Compose
                      transforms:
                      - _target_: pytorchvideo.transforms.ShortSideScale
                        size: 224  # 256
                      - _target_: torchvision.transforms.CenterCrop
                        size: 224
                      - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                        mean: [0.485, 0.456, 0.406]
                        std: [0.229, 0.224, 0.225]
                # Have to do this transform separately since SpatialCrop was written to
                # expect a list as input (hence can't wrap with a ListTransform)
                # TODO: Write a simpler version of spatial transform without expecting
                # lists and then just flatten the list of lists.
                - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                  base_transform:
                    _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                    crop_size: 224
                    num_crops: 3
            captions_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/captions.npy
            caption2data_mapping_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/MSR-VTT/1k-A/captions2audio.npy
            tokenizer: ${trainer.model.zero_shot_with_text_targets.audioset.label_strings.tokenizer}
          shuffle: False
          batch_size: 100
          num_workers: 10
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: msrvtt
          worker_init_fn: NULL
  model:
    _target_: omnivore.models.multimodal_wrapper.MultiModalZeroShotWithTextTargetsWrapper
    zero_shot_with_text_targets:
      in1k:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list:
              - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
      audioset:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/bpe_simple_vocab_16e6.txt.gz
              - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
          label_names_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/label_names.npy
          templates:
            _target_: omnivore.utils.data.FileLoader.load
            return_idx: False
            path_list:
              - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
      audioset_video: ${.audioset}
      esc_fold1:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/ESC-50/label_names.npy
          templates:  ${...audioset.label_strings.templates}
      vggsound:
        label_strings:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
          tokenizer: ${...audioset.label_strings.tokenizer}
          label_names_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/VGGSound/label_names.npy
          templates:  ${...audioset.label_strings.templates}
    multimodal_model:
      _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
      list_input_reduction: mean
      dataset_specific_list_input_reduction:
        # Keep audiocaps output (used in val) as is for retrieval eval
        # Very important, else it will eval on avg pooled features
        audiocaps:
          - field_name: text_embed
            reduction_op: no_op
        clotho: ${.audiocaps}
        msrvtt: ${.audiocaps}
      modality_preprocessors:
        - name: rgbdt_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
            img_size:
            - 3
            - ${video_num_frames}
            - 224
            - 224
            num_cls_tokens: 1
            pos_embed_fn:
              _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
              _partial_: true
              learnable: ${learnable_pos_rgbdt}
            depth_stem: NULL
            rgbt_stem:
              _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
              proj_stem:
                - _target_: omnivore.models.PadIm2Video
                  pad_type: repeat
                  ntimes: 2
                - _target_: torch.nn.Conv3d
                  in_channels: 3
                  kernel_size: ${rgb_kernel_size}
                  out_channels: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                  stride: ${.kernel_size}
                  bias: False
              norm_layer:
                _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
        - name: text_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.TextPreprocessor
            context_length: 77
            vocab_size: 49408
            embed_dim: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            causal_masking: True
        - name: audio_preprocessor
          preprocessor:
            _target_: omnivore.models.multimodal_preprocessors.AudioPreprocessor
            img_size:
            - 1
            - ${audio_num_mel_bins}
            - ${audio_target_len}
            num_cls_tokens: 1
            pos_embed_fn:
              _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
              _partial_: true
              learnable: ${learnable_pos_audio}
            audio_stem:
              _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
              proj_stem:
                - _target_: torch.nn.Conv2d
                  in_channels: 1
                  kernel_size: 16
                  out_channels: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                  stride: ${.kernel_size}
                  bias: False
              norm_layer:
                _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
      sample_to_modality_preprocessor:
        - sample_type: ${get_class:omnivore.data.api.BatchVisionTextSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbdt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchTextSample}
          sample_field_to_modality:
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbdt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchVisionAudioSample}
          sample_field_to_modality:
          - input_fields: ["vision"]
            preprocessor_name: rgbdt_preprocessor
            output_key: "vision_tokens"
            output_key_for_dict: False
          - input_fields: ["audio"]
            preprocessor_name: audio_preprocessor
            output_key: "audio_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchAudioSample}
          sample_field_to_modality:
          - input_fields: ["audio"]
            preprocessor_name: audio_preprocessor
            output_key: "audio_tokens"
            output_key_for_dict: False
        - sample_type: ${get_class:omnivore.data.api.BatchAudioTextSample}
          sample_field_to_modality:
          - input_fields: ["audio"]
            preprocessor_name: audio_preprocessor
            output_key: "audio_tokens"
            output_key_for_dict: False
          - input_fields: ["text"]
            preprocessor_name: text_preprocessor
            output_key: "text_tokens"
            output_key_for_dict: False
      trunks:
        - name: vision_text
          trunk:
            _target_: omnivore.models.simple_transformer.SimpleTransformer
            embed_dim: 768
            num_blocks: 12
            ffn_dropout_rate: 0.0
            drop_path_rate: 0.0 # OpenCLIP
            attn_target:
              _target_: omnivore.models.simple_transformer.MultiheadAttention
              embed_dim: ${..embed_dim}
              num_heads: 12
              dropout: 0.0
              bias: True
              add_bias_kv: True
              _partial_: True
            pre_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "b l d -> l b d"
            post_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "l b d -> b l d"
        - name: audio
          trunk:
            _target_: omnivore.models.simple_transformer.SimpleTransformer
            embed_dim: 768
            num_blocks: 12
            ffn_dropout_rate: 0.0
            drop_path_rate: 0.0 # OpenCLIP
            attn_target:
              _target_: omnivore.models.simple_transformer.MultiheadAttention
              embed_dim: ${..embed_dim}
              num_heads: 12
              dropout: 0.0
              bias: True
              add_bias_kv: True
              _partial_: True
            pre_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "b l d -> l b d"
            post_transformer_layer:
              _target_: omnivore.models.helpers.EinOpsRearrange
              rearrange_expr: "l b d -> b l d"
      tokens_to_trunks:
        - trunk_name: vision_text
          input_keys:
            - vision_tokens
            - text_tokens
        - trunk_name: audio
          input_keys:
            - audio_tokens
      heads:
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0 # select CLS token
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: ${.in_features}
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
            - _target_: torch.nn.ReLU
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${...2.model.out_features}
                out_features: ${embed_dim}
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
          fork_module: ""
          preprocessed_input_key: "vision_tokens"
          output_key: "vision_embed"
        - head:
            _target_: omnivore.models.pooling_helpers.SelectEOSAndProject
            proj:
              _target_: torch.nn.Sequential
              _args_:
              - _target_: torch.nn.LayerNorm # called self.ln_final in CLIP OpenCLIP
                normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
              - _target_: omnivision.model.model_init_utils.init_parameters
                model:
                  _target_: torch.nn.Linear
                  in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                  out_features: ${embed_dim}
                  bias: False # OpenCLIP
                init_fns:
                  weight:
                    _target_: torch.nn.init.normal_
                    _partial_: True
                    mean: 0
                    std: 0.03608 # 768 ** -0.5
          fork_module: ""
          preprocessed_input_key: "text_tokens"
          output_key: "text_embed"
        - head:
            _target_: torch.nn.Sequential
            _args_:
            - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
              normalized_shape: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
            - _target_: omnivore.models.pooling_helpers.SelectElement
              index: 0 # select CLS token
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${trainer.model.multimodal_model.trunks.0.trunk.embed_dim}
                out_features: ${.in_features}
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
            - _target_: torch.nn.ReLU
            - _target_: omnivision.model.model_init_utils.init_parameters
              model:
                _target_: torch.nn.Linear
                in_features: ${...2.model.out_features}
                out_features: ${embed_dim}
                bias: False # OpenCLIP
              init_fns:
                weight:
                  _target_: torch.nn.init.normal_
                  _partial_: True
                  mean: 0
                  std: 0.03608 # 768 ** -0.5
          fork_module: ""
          preprocessed_input_key: "audio_tokens"
          output_key: "audio_embed"
      postprocessors:
        - name: "normalize"
          postprocessor:
            _target_: omnivore.models.helpers.Normalize
            dim: -1
        - name: "normalize_and_scale_fixed"
          postprocessor:
            _target_: torch.nn.Sequential
            _args_:
              - _target_: omnivore.models.helpers.Normalize
                dim: -1
              - _target_: omnivore.models.helpers.LearnableLogitScaling
                learnable: False
      head_to_postprocessor:
        - input_key: "vision_embed"
          postprocessor_name: "normalize"
        - input_key: "text_embed"
          postprocessor_name: "normalize_and_scale_fixed"
        - input_key: "audio_embed"
          postprocessor_name: "normalize_and_scale_fixed"
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.95
      eps: 1e-6
    gradient_clip: NULL
    amp:
      enabled: True
      amp_dtype: bfloat16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1.6e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1.6e-4
            lengths:
              - ${divide:${warmup_epochs},${trainer.max_epochs}}
              - ${subtract:1,${divide:${warmup_epochs},${trainer.max_epochs}}}
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.2
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*.bias'
            - '*pos_embed'
            - '*cls_token'
          module_cls_names: ["torch.nn.LayerNorm"]

  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
      audioset:
        mAP:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
        knn:
          _target_: omnivore.meters.knn_accuracy.KnnAccuracy
          feat_name: audio_embed
          topks: [10, 20]
          multilabel_mode: recall
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
            multilabel_mode: recall
      audioset_video:
        mAP: ${..audioset.mAP}
        knn:
          _target_: omnivore.meters.knn_accuracy.KnnAccuracy
          feat_name: vision_embed
          topks: [10, 20]
          multilabel_mode: recall
        accuracy_top5: ${..audioset.accuracy_top5}
      esc_fold1: ${.in1k}
      vggsound: ${.in1k}
      audiocaps:
        recall_text2vid:
          _target_: omnivore.meters.cross_modality_retrieval.CrossModalityRetrieval
          query_feature: text_embed
          corpus_feature: audio_embed
          topks: [1, 10]
      clotho: ${.audiocaps}
      msrvtt:
        recall_text2vid:
          _target_: omnivore.meters.cross_modality_retrieval.CrossModalityRetrieval
          query_feature: text_embed
          corpus_feature: vision_embed
          topks: [1, 5, 10]
  loss:
    laion:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: vision_embed
      feat2_name: text_embed
      logit_scale_name: NULL
      normalize: False # OpenClip normalizes outputs in the model
    audioset:
      _target_: omnivore.losses.scaled_loss.ScaledLoss
      scale: 1.0
      loss_fn:
        _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
        feat1_name: vision_embed
        feat2_name: audio_embed
        logit_scale_name: NULL
        normalize: False # OpenClip normalizes outputs in the model

  distributed:
    backend: nccl
    comms_dtype: bfloat16
    find_unused_parameters: True

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir: ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10
    # tensorboard_embedding_writer:
    #   _target_: omnivore.logger.TensorBoardEmbeddingLogger
    #   path: ${..tensorboard_writer.log_dir}

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True

launcher:
  num_nodes: 4
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .


submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
