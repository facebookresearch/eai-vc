# @package _global_

num_audioset_classes: 527

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 60
  mode: train
  accelerator: cuda
  seed_value: 42
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.AudioPathDataset
        num_mel_bins: 128
        target_length: 1024
        label_type: csv
        new_prefix: /datasets01/audioset/042319/data/
        path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/balanced_train_segments_filelist.npy
        label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/balanced_train_segments_labels.npy
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchaudio.transforms.FrequencyMasking
                  freq_mask_param: 48
                - _target_: torchaudio.transforms.TimeMasking
                  time_mask_param: 192
                - _target_: torchvision.transforms.Normalize
                  # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                  mean: -4.268
                  std: 4.569
      shuffle: True
      batch_size: 32
      num_workers: 10
      pin_memory: True
      drop_last: True
      # TODO AST does mixup before kaldi and other transforms.. need to figure what is the right way
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        convert_label_to_one_hot_num_classes: 527
        # batch_kwargs:
        #     model_fwd_kwargs:
        #       use_checkpoint: True
        output_key: audioset
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.AudioPathDataset
        num_mel_bins: ${trainer.data.train.dataset.num_mel_bins}
        target_length: ${trainer.data.train.dataset.target_length}
        label_type: ${trainer.data.train.dataset.label_type}
        new_prefix: /datasets01/audioset/042319/data/
        path_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_filelist.npy
        label_file_list:
          - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/eval_segments_labels.npy
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: audio
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.Normalize
                  # From table 3 https://arxiv.org/pdf/2207.06405.pdf or https://github.com/YuanGongND/ast/blob/d7d8b4b8e06cdaeb6c843cdb38794c1c7692234c/src/run.py#L62
                  mean: -4.268
                  std: 4.569
      shuffle: False
      batch_size: 32
      num_workers: 8
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: audioset
      worker_init_fn: NULL

  model:
    _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
    strict: False # heads aren't loaded
    state_dict:
      _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
      checkpoint_path: /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/2021_DeiT/deit_base_distilled_patch16_384-d0272ac0.pth
      ckpt_state_dict_keys: ["model"]
      checkpoint_kernels:
      - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
        source_pattern: ""
        target_pattern: "trunk."
        key_pattern: NULL
      - _target_: omnivision.model.checkpoint_utils.CkptProcessKernel
        key_pattern:
          - trunk.patch_embed.proj.weight
        processor:
          _target_: torch.mean
          dim: 1
          keepdims: true
          _partial_: True
      - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
        key_pattern:
        # - "trunk.decoder.*"
        # - "trunk.norm.*"
        # - "trunk.mask_token"
        - "trunk.dist_token"  # distillation token
        - "trunk.head_dist*"  # distillation head
        - "trunk.head.*"
        - "trunk.pos_embed"  # TODO Ideally need to interpolate this and load
        # - "trunk.patch_embed.*"
    model:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      handle_list_inputs: False
      trunk:
        _target_: omnivore.models.vision_transformer.VisionTransformer
        img_size:
          - 1
          - ${trainer.data.train.dataset.num_mel_bins}
          - ${trainer.data.train.dataset.target_length}
        embed_dim: 768
        depth: 12
        patch_size: [16, 16]
        drop_path_rate: 0.1
        patch_embed_type: generic
        patch_embed_params_list:
        - _target_: omnivore.models.make_conv_or_linear
          layer:
            _target_: torch.nn.Conv2d
            in_channels: 1
            out_channels: ${....embed_dim}
            kernel_size: ${....patch_size}
            stride: ${.kernel_size}
          init_weight:
            _target_: omnivore.models.reshape_and_init_as_mlp
          _recursive_: False
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 12
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
      heads:
        - head:
            _target_: omnivision.model.model_init_utils.init_parameters
            model:
              _target_: torch.nn.Linear
              in_features: 768
              out_features: ${num_audioset_classes}
            init_fns:
              weight:
                _target_: timm.models.layers.trunc_normal_
                _partial_: True
                mean: 0
                std: 2e-5
              bias:
                _target_: torch.nn.init.zeros_
                _partial_: True
          fork_module: ""
          input_key: audioset
          output_key: audioset
      trunk_fields:
        - input_key: NULL
          args: ["audio"]

  optim:
    amp:
      enabled: True
      amp_dtype: float16  # bfloat16 or float16
    gradient_clip: NULL
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.95
    param_group_modifiers:
      - _target_: omnivore.optim.layer_decay_param_modifier.layer_decay_param_modifier
        _partial_: True
        layer_decay_value: 0.65
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-6
            lengths: [0.07, 0.93]  # warm for 4 epochs
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 1e-4
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
             - '*.bias'
            #  - '*.pos_embed'
            #  - '*.cls_token'
          module_cls_names: ['torch.nn.LayerNorm']
  meters:
    # mAP during train takes too much GPU mem due to storing all state
    # train:
    #   audioset:
    #     mAP:
    #       _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
    val:
      audioset:
        mAP:
          _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
  loss:
    audioset:
      _target_: torch.nn.BCEWithLogitsLoss

  distributed:
    backend: nccl
    comms_dtype: float16
    find_unused_parameters: false

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True

launcher:
  num_nodes: 1
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: audioset
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
