# @package _global_

data_module:
  _target_: omnivision.data_module.base_data_module.BaseDataModule
  train:
    _target_: omnivore.data.torch_dataset.TorchDataset
    dataset:
      _target_: omnivore.data.path_dataset.VideoPathDataset
      path_file_list:
        - manifold://omnivore/tree/datasets/SSv2/lists/vidpaths_train.npy
      label_file_list:
        - manifold://omnivore/tree/datasets/SSv2/lists/labels_train.npy
      clip_sampler:
        _target_: pytorchvideo.data.clip_sampling.RandomClipSampler
        clip_duration: 2.7
      frame_sampler:
        _target_: pytorchvideo.transforms.UniformTemporalSubsample
        num_samples: 16
      decoder: pyav
      normalize_to_0_1: True
      transforms:
        - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
          base_transform:
            _target_: torchvision.transforms.Compose
            transforms:
            - _target_: pytorchvideo.transforms.ShortSideScale
              size: 256
            - _target_: torchvision.transforms.RandomResizedCrop
              size: 224
            # - _target_: torchvision.transforms.RandomHorizontalFlip
            #   p: 0.5
            - _target_: omnivore.data.transforms.basic.Permute
              ordering: [1, 0, 2, 3]  # C,T,H,W -> T,C,H,W for RandAug
            - _target_: pytorchvideo.transforms.RandAugment
              magnitude: 7
              num_layers: 4
              prob: 0.5
            - _target_: omnivore.data.transforms.basic.Permute
              ordering: [1, 0, 2, 3]  # T,C,H,W -> C,T,H,W after RandAug
            - _target_: torchvision.transforms._transforms_video.NormalizeVideo
              mean: [0.485, 0.456, 0.406]
              std: [0.229, 0.224, 0.225]
            - _target_: omnivore.data.transforms.basic.Permute
              ordering: [1, 0, 2, 3]  # C,T,H,W -> T,C,H,W for cube RandErase over batch(time) dim
            - _target_: omnivore.data.transforms.video_random_erasing.RandomErasing
              probability: 0.25
              mode: pixel
              max_count: 1
              num_splits: 1
              cube: True
              device: cpu
            - _target_: omnivore.data.transforms.basic.Permute
              ordering: [1, 0, 2, 3]  # C,T,H,W -> T,C,H,W after RandErase
    shuffle: True
    batch_size: 8
    num_workers: 8
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: omnivore.data.api.DefaultOmnivoreCollator
      output_key: ssv2
      batch_transforms:
        - _target_: omnivore.data.transforms.cutmixup.CutMixUp
          mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
          cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
          prob: 1.0 # probability of applying mixup or cutmix per batch or element
          switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
          mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
          correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
          label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
          num_classes: 174 # number of classes for target
    worker_init_fn: NULL
  val:
    _target_: omnivore.data.torch_dataset.TorchDataset
    dataset:
      _target_: omnivore.data.path_dataset.VideoPathDataset
      path_file_list:
        - manifold://omnivore/tree/datasets/SSv2/lists/vidpaths_validation.npy
      label_file_list:
        - manifold://omnivore/tree/datasets/SSv2/lists/labels_validation.npy
      clip_sampler:
        _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
        clip_duration: ${data_module.train.dataset.clip_sampler.clip_duration}
        clips_per_video: 5
      frame_sampler:
        _target_: pytorchvideo.transforms.UniformTemporalSubsample
        num_samples: ${data_module.train.dataset.frame_sampler.num_samples}
      decoder: ${data_module.train.dataset.decoder}
      normalize_to_0_1: ${data_module.train.dataset.normalize_to_0_1}
      transforms:
        - _target_: torchvision.transforms.Compose
          transforms:
            # Not splitting into multiple sample objects to keep the crops of one video
            # in 1 sigle Sample object
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                base_transform:
                  _target_: pytorchvideo.transforms.ShortSideScale
                  size: 224
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                base_transform:
                  _target_: torchvision.transforms._transforms_video.NormalizeVideo
                  mean: ${data_module.train.dataset.transforms.0.base_transform.transforms.5.mean}
                  std: ${data_module.train.dataset.transforms.0.base_transform.transforms.5.std}
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                base_transform:
                  _target_: omnivore.data.transforms.basic.SpatialCrop
                  crop_size: 224
                  num_crops: 3
            # Each sample now has a list in the .data object, so convert into sublists
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: omnivore.data.transforms.transform_wrappers.FlattenListOfList
    shuffle: False
    batch_size: 1
    num_workers: 8
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: omnivore.data.api.DefaultOmnivoreCollator
      output_key: ssv2
    worker_init_fn: NULL


lightning_module:
  _target_: omnivore.lightning_module.omnivore_lightning_module.OmnivoreLightningModule
  model:
    _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
    strict: False # heads aren't loaded
    state_dict:
      _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
      checkpoint_path: manifold://omnivore/tree/rgirdhar/omnivision_omnivore/config/experiments/rgirdhar/0016_mae_vitbase_concat.yaml/0/checkpoints/last.ckpt
      ckpt_state_dict_keys: ["state_dict"]
      checkpoint_kernels:
      - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
        source_pattern: "model."
        target_pattern: ""
        key_pattern: NULL
        # - "model.trunk.*"
      - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
        key_pattern:
        - "trunk.decoder.*"
        - "trunk.norm.*"
        - "trunk.mask_token"
        - "heads.*"

    model:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      handle_list_inputs: True
      trunk:
        _target_: omnivore.models.vision_transformer.VisionTransformer
        img_size:
          - 3
          - ${data_module.train.dataset.frame_sampler.num_samples}
          - 224
          - 224
        embed_dim: 768
        depth: 12
        patch_size: [2, 16, 16]
        classifier_feature: global_pool
        drop_path_rate: 0.1
        use_cls_token: False
        patch_embed_type: generic
        patch_embed_params_list:
        - _target_: omnivore.models.PadIm2Video
          pad_type: repeat
          ntimes: 2
        - _target_: omnivore.models.make_conv_or_linear
          layer:
            _target_: torch.nn.Conv3d
            in_channels: 3
            out_channels: ${....embed_dim}
            kernel_size: ${....patch_size}
            stride: ${.kernel_size}
          init_weight:
            _target_: omnivore.models.reshape_and_init_as_mlp
          _recursive_: False
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 12
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
        learnable_pos_embed: False  # Use sinusoidal positional encoding
      heads:
        - head:
            _target_: torch.nn.Sequential
            _args_:
              # - _target_: torch.nn.Dropout
              #   p: 0.0
              - _target_: omnivision.model.model_init_utils.init_parameters
                model:
                  _target_: torch.nn.Linear
                  in_features: 768  # 8 * 96
                  out_features: 174
                init_fns:
                  weight:
                    _target_: torch.nn.init.normal_
                    _partial_: True
                    mean: 0
                    std: 0.01
                  bias:
                    _target_: torch.nn.init.zeros_
                    _partial_: True
          fork_module: ""
          input_key: NULL
          output_key: NULL
      trunk_fields:
        - input_key: NULL
          args: ["vision"]
  optim:
    optimizer:
      _target_: torch.optim.AdamW
    param_group_modifiers:
      - _target_: omnivore.optim.layer_decay_param_modifier.layer_decay_param_modifier
        _partial_: True
        layer_decay_value: 0.75
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-6
            lengths: [0.125, 0.875]  # 5 epoch warmup
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
             - '*.bias'
             # - '*.pos_embed'  # using sinusoidal
             #  - '*.cls_token'  # no cls token
          module_cls_names: ['torch.nn.LayerNorm']
  meters:
    train:
      ssv2:
        accuracy_top1:
          _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 5
    val:
      ssv2:
        accuracy_top1:
          _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 5
  loss:
    ssv2:
      _target_: omnivore.losses.cross_entropy_multiple_output_single_target.CrossEntropyMultipleOutputSingleTargetLoss
      ignore_index: -1
      update_output_apply_activation: True

lightning_trainer:
  _target_: pytorch_lightning.Trainer
  num_nodes: ${launcher.num_nodes}
  gpus: ${launcher.gpus_per_node}
  sync_batchnorm: False
  replace_sampler_ddp: False
  #limit_train_batches: 2
  #limit_val_batches: 2
  #limit_test_batches: 0
  max_epochs: 40
  accelerator: ${launcher.accelerator}
  strategy: ${launcher.strategy}

launcher:
  num_nodes: 8
  gpus_per_node: 8
  mode: train
  accelerator: gpu
  strategy: ddp
  experiment_log_dir: .
