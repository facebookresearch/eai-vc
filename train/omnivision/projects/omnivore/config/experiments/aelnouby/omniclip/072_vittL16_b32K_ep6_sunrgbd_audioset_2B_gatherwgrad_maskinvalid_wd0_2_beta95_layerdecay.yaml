# @package _global_

defaults:
  - /experiments/aelnouby/omniclip/064_vittL16_b32K_ep6_sunrgbd_audioset_2B_gatherwgrad_maskinvalid_wd0_2_beta95

trainer:
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.95
      eps: 1e-6
    gradient_clip:
      _target_: omnivore.optim.helpers.GradientClipper
      max_norm: 5.0
      norm_type: 2
    # gradient_logger:
    #   _target_: omnivore.optim.monitoring.GradientNormWatcher
    #   output_path: ${launcher.experiment_log_dir}/gradients.json
    #   accum_steps: 10
    #   detailed: True
    amp:
      enabled: True
      amp_dtype: bfloat16 # bfloat16 or float16
    param_group_modifiers:
      - _target_: omnivore.optim.layer_decay_param_modifier.layer_decay_param_modifier
        _partial_: True
        layer_decay_value: 0.9
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-5
            lengths:
              - ${divide:${constants.warmup_epochs},${trainer.max_epochs}}
              - ${subtract:1,${divide:${constants.warmup_epochs},${trainer.max_epochs}}}
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.2
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*.bias'
            - '*pos_embed'
            - '*cls_token'
            - "*log_logit_scale"
          module_cls_names: ["torch.nn.LayerNorm"]
