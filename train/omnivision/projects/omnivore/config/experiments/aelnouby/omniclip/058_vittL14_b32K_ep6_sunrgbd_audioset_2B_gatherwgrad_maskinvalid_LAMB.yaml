# @package _global_

defaults:
  - /experiments/aelnouby/omniclip/056_vittL14_b32K_ep6_sunrgbd_audioset_2B_gatherwgrad_maskinvalid


constants:
  batch_size: 448 # OOM when 512

trainer:
  optim:
    optimizer:
      _target_: apex.optimizers.FusedLAMB
      betas:
        - 0.9
        - 0.98
      eps: 1e-6
      max_grad_norm: 1.0
    # gradient_clip:
    #   _target_: omnivore.optim.helpers.GradientClipper
    #   max_norm: 1.0
    #   norm_type: 2
    # gradient_logger:
    #   _target_: omnivore.optim.monitoring.GradientNormWatcher
    #   output_path: ${launcher.experiment_log_dir}/gradients.json
    #   accum_steps: 10
    #   detailed: True
    amp:
      enabled: True
      amp_dtype: bfloat16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 0
            lengths: [0.01, 0.99]
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            - '*.bias'
            - '*pos_embed'
            - '*cls_token'
            - "*log_logit_scale"
          module_cls_names: ["torch.nn.LayerNorm"]

  