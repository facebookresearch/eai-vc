# @package _global_

# Follow projects/omnivore/config/experiments/aelnouby/omniclip/003_im1k_k400_concat_txt_vitbase_16nodes.yaml
# Use AMP for optimization
# Use FP16 gradient comm
# Use Omnivision Trainer

base_batchsize_per_replica: 256

trainer:
  _target_: omnivore.omnivision_trainer.OmnivisionTrainer
  max_epochs: 100
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      repeat_factors: [1.0, 1.0]
      datasets:
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list:
              - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz  # AWS
              - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
                /vidpaths_train.npy
              - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
          label_names_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
          templates:
            - "{}"
          base_dataset:
            _target_: omnivore.data.path_dataset.ImagePathDataset
            path_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/train_images_global.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_images_manifold_v2.npy
            label_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/train_labels.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_labels.npy
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.RandomResizedCrop
                      size: 112
                      interpolation: 3
                    - _target_: torchvision.transforms.RandomHorizontalFlip
                    - _target_: omnivore.data.transforms.rand_auto_aug.RandAugment  # Essentially autoagument rand-m9-mstd0.5-inc1
                      magnitude: 9
                      magnitude_std: 0.5
                      increasing_severity: True
                    - _target_: torchvision.transforms.ColorJitter
                      brightness: 0.4
                      contrast: 0.4
                      saturation: 0.4
                      hue: 0.4
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.RandomErasing
                      p: .25
                    - _target_: torchvision.transforms.Normalize
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
                    # - _target_: omnivore.data.transforms.image_video.ImageToSingleFrameVideo
        shuffle: True
        batch_size: ${base_batchsize_per_replica}
        num_workers: 10
        pin_memory: True
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
          tokenizer:
            _target_: slip.tokenizer.SimpleTokenizer
            bpe_path_list:
              - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
              - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
              - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
          label_names_file_list:
            - /checkpoint/aelnouby/datasets/k400_label_names.npy   # FAIR / AWS 
            - manifold://omnivore/tree/datasets/kinetics_400_meta/label_names.npy
          templates:
            - "{}"
          base_dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics/400/vidpaths_train.npy
              - /checkpoint/aelnouby/datasets/k400/vidpaths_train.npy     # AWS
              - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_train.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics/400/labels_train.npy
              - /checkpoint/aelnouby/datasets/k400/labels_train.npy       # AWS
              - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_train.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.RandomClipSampler
              clip_duration: 2
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: 16
            decoder: pyav
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                  - _target_: pytorchvideo.transforms.ShortSideScale
                    size: 128
                  - _target_: torchvision.transforms.RandomResizedCrop
                    size: 112
                  - _target_: torchvision.transforms.RandomHorizontalFlip
                    p: 0.5
                  - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]

        shuffle: True
        batch_size: ${base_batchsize_per_replica}
        num_workers: 3
        pin_memory: True
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: k400
          batch_kwargs:
            model_fwd_kwargs:
              use_checkpoint: True
            accum_steps: 2
        worker_init_fn: NULL
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
          tokenizer: ${trainer.data.train.datasets.0.dataset.tokenizer}
          label_names_file_list: ${trainer.data.train.datasets.0.dataset.label_names_file_list}
          templates: ${trainer.data.train.datasets.0.dataset.templates}
          base_dataset:
            _target_: omnivore.data.path_dataset.ImagePathDataset
            path_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
            label_file_list:
              - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
              - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                    - _target_: torchvision.transforms.Resize
                      size: 112
                      interpolation: 3
                    - _target_: torchvision.transforms.CenterCrop
                      size: 112
                    - _target_: torchvision.transforms.ToTensor
                    - _target_: torchvision.transforms.Normalize
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
        shuffle: False
        batch_size: ${base_batchsize_per_replica}
        num_workers: 10
        pin_memory: True
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
        worker_init_fn: NULL
      - _target_: omnivore.data.torch_dataset.TorchDataset
        dataset:
          _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
          tokenizer: ${trainer.data.train.datasets.1.dataset.tokenizer}
          label_names_file_list: ${trainer.data.train.datasets.1.dataset.label_names_file_list}
          templates: ${trainer.data.train.datasets.1.dataset.templates}
          base_dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/vidpaths_val.npy
              - /checkpoint/aelnouby/datasets/k400/vidpaths_val.npy   # AWS
              - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/labels_val.npy
              - /checkpoint/aelnouby/datasets/k400/labels_val.npy     # AWS
              - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
            #name: k400_val
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: 10
              clips_per_video: 1
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: 160
            decoder: pyav
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: torchvision.transforms.Compose
                  transforms:
                  - _target_: pytorchvideo.transforms.ShortSideScale
                    size: 112
                  - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]
                  - _target_: omnivore.data.transforms.pytorchvideo.TemporalCrop
                    frames_per_clip: 16
                    stride: 40
                  - _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                    crop_size: 112
                    num_crops: 3

        # FIX shuffle false getting ignored
        shuffle: False
        batch_size: 4
        num_workers: 8
        pin_memory: True
        drop_last: True
        collate_fn:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: k400
        worker_init_fn: NULL


  model:
    _target_: omnivision.model.model_wrappers.MultiModalZeroShotEvalWrapper
    logit_scale_output_key: "logit_scale"
    vision_trunk:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      handle_list_inputs: True
      trunk:
        _target_: omnivore.models.vision_transformer.VisionTransformer
        embed_dim: 384
        img_size:
          - 3
          - ${trainer.data.train.datasets.1.dataset.base_dataset.frame_sampler.num_samples}
          - 112
          - 112
        patch_size:
          - 2
          - 16
          - 16
        patch_embed_type: generic
        patch_embed_params_list:
          - _target_: omnivore.data.transforms.image_video.RepeatedPadIm2VideoSingleImage
            ntimes: 2
            time_dim: 2
          - _target_: torch.nn.Conv3d
            in_channels: 3
            out_channels: ${...embed_dim}
            kernel_size: ${...patch_size}
            stride:  ${...patch_size}
        depth: 12
        drop_path_rate: 0.0
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 12
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
      heads:
        - fork_module: ""
          head:
            _target_: torch.nn.Linear
            in_features: ${trainer.model.vision_trunk.trunk.embed_dim}
            out_features: 512
          input_key: NULL
          output_key: image_embed
      trunk_fields:
        - input_key: NULL
          args: ["vision"]
    text_trunk:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      trunk:
        _target_: omnivore.models.slip_text_transformer.SLIPTextTransformer
        context_length: 77
        vocab_size: 49408
        transformer_width: 512
        transformer_heads: 8
        transformer_layers: 12
      heads:
        - fork_module: ""
          head:
            _target_: torch.nn.Linear
            in_features: ${trainer.model.text_trunk.trunk.transformer_width}
            out_features: 512
          input_key: NULL
          output_key: text_embed
      trunk_fields:
        - input_key: NULL
          args: ["text"]
    label_strings:
      in1k:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${trainer.data.train.datasets.0.dataset.tokenizer}
        label_names_file_list: ${trainer.data.train.datasets.0.dataset.label_names_file_list}
        templates: ${trainer.data.train.datasets.0.dataset.templates}
      k400:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${trainer.data.train.datasets.0.dataset.tokenizer}
        label_names_file_list: ${trainer.data.train.datasets.1.dataset.label_names_file_list}
        templates: ${trainer.data.train.datasets.1.dataset.templates}

  optim:
    optimizer:
      _target_: torch.optim.AdamW
      # betas: [0.9, 0.95]
      # eps: 1e-8
    gradient_clip: NULL
    amp:
      enabled: True
      amp_dtype: float16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 4e-4  # 5e-4 for 4K; 2x this LR for 512
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-6
            lengths: [0.1, 0.9]  # warm for 5 epochs
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            # - 'logit_scale'
            - '*bias*'
            - '*.pos_embed'
            - '*.cls_token'
            # - '*ln*'
            # - 'visual*norm*'
          # TODO: allow other forms for class names
          module_cls_names: ['torch.nn.LayerNorm']
  metrics:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.metrics.accuracy.Accuracy
          top_k: 1
        accuracy_top5:
          _target_: omnivore.metrics.accuracy.Accuracy
          top_k: 5
      k400:
        accuracy_top1:
          _target_: omnivore.metrics.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivore.metrics.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 5
  loss:
    k400:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLossLegacy
      feat1_name: ${trainer.model.vision_trunk.heads.0.output_key}
      feat2_name: ${trainer.model.text_trunk.heads.0.output_key}
      learnable_temperature: False
      temperature: 1.0
      all_gather_fn:
        _target_: slip.utils.all_gather_batch_with_grad
        _partial_: True
    in1k: ${.k400}



  distributed:
    backend: nccl
    amp_comms_type: float16
    find_unused_parameters: True

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 1 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True

launcher:
  num_nodes: 2
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  #constraints: "volta32gb" 
  port_range: [10000, 65000]
