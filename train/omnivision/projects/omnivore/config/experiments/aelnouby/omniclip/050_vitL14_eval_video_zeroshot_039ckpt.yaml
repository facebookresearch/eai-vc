# @package _global_

# Follow config/experiments/imisra/014_oclip_vitb32_laion400mact_imval_b32K_lr5e-4_adampt98_ep32_wdpt2_nocoloraug.yaml
# Use AMP for optimization
# Use FP16 gradient comm
# Use Omnivision Trainer


video_clip_duration: 10
# The 102.4 (could have been 100) is for historical reasons where we used 1024
# target length for 10s clip as per AST
video_num_frames: ${video_clip_duration}
embed_dim: 512

constants:
  kernel_size: 14
  batch_size: 4
  head_final_embed_dim: 512
  vision_pos_embed_learnable: True


trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 32
  mode: val
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    val:
      _target_: omnivore.data.concat_dataset.ConcatDataset
      max_steps: sum
      datasets:
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
            _target_: omnivore.data.path_dataset.VideoPathDataset
            decode_audio: False
            label_type: csv
            remove_prefix: eval_segments/video/
            new_prefix: /fsx-omnivore/rgirdhar/data/audioset/eval_segments/video_mp4-288p/
            path_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_filelist.npy
            label_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSetVideo/eval_segments_labels.npy
            clip_sampler:
              _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
              clip_duration: ${video_clip_duration}
              clips_per_video: ${ceil_int:${divide:10,${video_clip_duration}}}
            frame_sampler:
              _target_: pytorchvideo.transforms.UniformTemporalSubsample
              num_samples: 160
            decoder: decord  # since this allows for audio decoding
            normalize_to_0_1: True
            transforms:
              - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                base_transform:
                  _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                  base_transform:
                    _target_: torchvision.transforms.Compose
                    transforms:
                    - _target_: pytorchvideo.transforms.ShortSideScale
                      size: 224  # 256
                    - _target_: torchvision.transforms.CenterCrop
                      size: 224
                    - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                      mean: [0.485, 0.456, 0.406]
                      std: [0.229, 0.224, 0.225]
          shuffle: False
          batch_size: ${constants.batch_size}  # 128 was on cusp of OOM sometimes
          num_workers: 10
          pin_memory: True
          drop_last: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: audioset_video
            convert_label_to_one_hot_num_classes: 527
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: False
          worker_init_fn: NULL
        - _target_: omnivore.data.torch_dataset.TorchDataset
          dataset:
              _target_: omnivore.data.path_dataset.VideoPathDataset
              path_file_list:
                - /checkpoint/aelnouby/datasets/k400/vidpaths_val.npy
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/vidpaths_val.npy
                - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
              label_file_list:
                - /checkpoint/aelnouby/datasets/k400/labels_val.npy
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/labels_val.npy
                - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
              #name: k400_val
              clip_sampler:
                _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
                clip_duration: ${video_clip_duration}
                clips_per_video: 1
              frame_sampler:
                _target_: pytorchvideo.transforms.UniformTemporalSubsample
                num_samples: 160
              decoder: pyav
              normalize_to_0_1: True
              transforms:
                  - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                    base_transform:
                      _target_: omnivore.data.transforms.transform_wrappers.ListTransform
                      base_transform:
                        _target_: torchvision.transforms.Compose
                        transforms:
                        - _target_: pytorchvideo.transforms.ShortSideScale
                          size: 224
                        - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                          mean: [0.485, 0.456, 0.406]
                          std: [0.229, 0.224, 0.225]
                  # Have to do this transform separately since SpatialCrop was written to
                  # expect a list as input (hence can't wrap with a ListTransform)
                  # TODO: Write a simpler version of spatial transform without expecting
                  # lists and then just flatten the list of lists.
                  - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
                    base_transform:
                      _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                      crop_size: 224
                      num_crops: 3
          # FIX shuffle false getting ignored
          shuffle: False
          batch_size: 4
          num_workers: 8
          pin_memory: True
          drop_last: False
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: False
            output_key: k400
          worker_init_fn: NULL
  model:  
    _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
    strict: False # heads aren't loaded
    state_dict:
      _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
      ckpt_state_dict_keys: ["model"]
      checkpoint_path: /fsx-omnivore/aelnouby/omnivision_omnivore/config/experiments/aelnouby/omniclip/039_cpy_034_warmup_fix_lr1e-3_eqbatch_outdim_dispmaxnorm_centering_detachSun_vitL14.yaml/0/checkpoints/checkpoint.pt 
      checkpoint_kernels:
      - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
        key_pattern:
        - "multimodal_model.modality_preprocessors.d_preprocessor.*"
        - "multimodal_model.postprocessors.normalize_and_scale_depth.*"
        - "multimodal_model.heads.1.*"
      - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
        source_pattern: "multimodal_model.modality_preprocessors.rgbt_preprocessor.rgbt_stem.proj."
        target_pattern: "multimodal_model.modality_preprocessors.rgbt_preprocessor.rgbt_stem.proj.1."
        key_pattern: NULL
      - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
        source_pattern: "multimodal_model.heads.2"
        target_pattern: "multimodal_model.heads.1"
        key_pattern: NULL
      - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
        source_pattern: "multimodal_model.postprocessors.normalize.center"
        target_pattern: "multimodal_model.postprocessors.normalize.2.center"
        key_pattern: NULL
    model:
      _target_: omnivore.models.multimodal_wrapper.MultiModalZeroShotWithTextTargetsWrapper
      zero_shot_with_text_targets:
        audioset_video:
          label_strings:
            _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
            tokenizer:
              _target_: slip.tokenizer.SimpleTokenizer
              bpe_path_list:
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/bpe_simple_vocab_16e6.txt.gz
                - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
            label_names_file_list:
              - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/AudioSet/label_names.npy
            templates:
              _target_: omnivore.utils.data.FileLoader.load
              return_idx: False
              path_list:
                - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
                - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        k400:
          label_strings:
            _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
            tokenizer:
              _target_: slip.tokenizer.SimpleTokenizer
              bpe_path_list:
                - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Pretrained/bpe_simple_vocab_16e6.txt.gz
                - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
            label_names_file_list:
              - /checkpoint/aelnouby/datasets/k400_label_names.npy   # FAIR / AWS 
              - manifold://omnivore/tree/datasets/kinetics_400_meta/label_names.npy
            templates:
              _target_: omnivore.utils.data.FileLoader.load
              return_idx: False
              path_list:
                - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
                - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
      multimodal_model:
        _target_: omnivore.models.multimodal_wrapper.MultimodalWrapper
        list_input_reduction: mean
        modality_preprocessors:
          - name: "rgbt_preprocessor"
            preprocessor:
                _target_: omnivore.models.multimodal_preprocessors.RGBDTPreprocessor
                img_size:
                - 3
                - 1
                - 224
                - 224
                num_cls_tokens: 1
                pos_embed_fn:
                  _target_: omnivore.models.multimodal_preprocessors.SpatioTemporalPosEmbeddingHelper
                  _partial_: true
                  learnable: ${constants.vision_pos_embed_learnable}
                depth_stem: NULL
                rgbt_stem:
                  _target_: omnivore.models.multimodal_preprocessors.PatchEmbedGeneric
                  proj_stem:
                  - _target_: torch.nn.Sequential
                    _args_:
                    - _target_: omnivore.models.helpers.EinOpsRearrange
                      rearrange_expr: "b c t h w -> (b t) c h w"
                    - _target_: torch.nn.Conv2d
                      kernel_size: ${constants.kernel_size}
                      in_channels: 3
                      out_channels: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
                      stride: ${.kernel_size}
                      bias: False
                  norm_layer:
                    _target_: torch.nn.LayerNorm # called self.ln_pre in VisualTransformer OpenCLIP
                    normalized_shape: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
          - name: text_preprocessor
            preprocessor:
              _target_: omnivore.models.multimodal_preprocessors.TextPreprocessor
              context_length: 77
              vocab_size: 49408
              embed_dim: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
              causal_masking: False
        sample_to_modality_preprocessor:
          - sample_type: ${get_class:omnivore.data.api.BatchVisionTextSample}
            sample_field_to_modality:
            - input_fields: ["vision"]
              preprocessor_name: rgbt_preprocessor
              output_key: "vision_tokens"
              output_key_for_dict: False
            - input_fields: ["text"]
              preprocessor_name: text_preprocessor
              output_key: "text_tokens"
              output_key_for_dict: False
          - sample_type: ${get_class:omnivore.data.api.BatchTextSample}
            sample_field_to_modality:
            - input_fields: ["text"]
              preprocessor_name: text_preprocessor
              output_key: "text_tokens"
              output_key_for_dict: False
          - sample_type: ${get_class:omnivore.data.api.BatchVisionSample}
            sample_field_to_modality:
            - input_fields: ["vision"]
              preprocessor_name: rgbt_preprocessor
              output_key: "vision_tokens"
              output_key_for_dict: False
        trunks:
          - name: vision
            trunk:
              _target_: omnivore.models.simple_transformer.SimpleTransformer
              embed_dim: 1024
              num_blocks: 24
              ffn_dropout_rate: 0.0
              drop_path_rate: 0.0 # OpenCLIP
              layer_scale_type: per_channel
              layer_scale_init_value: 0.0001
              attn_target:
                _target_: omnivore.models.simple_transformer.MultiheadAttention
                embed_dim: ${..embed_dim}
                num_heads: 16
                dropout: 0.0
                bias: True
                add_bias_kv: True
                _partial_: True
              pre_transformer_layer:
                _target_: omnivore.models.helpers.EinOpsRearrange
                rearrange_expr: "b l d -> l b d"
              post_transformer_layer:
                _target_: omnivore.models.helpers.EinOpsRearrange
                rearrange_expr: "l b d -> b l d"
        tokens_to_trunks:
          - trunk_name: vision
            input_keys:
              - vision_tokens
              - text_tokens
        heads:
          - head:
              _target_: torch.nn.Sequential
              _args_:
              - _target_: torch.nn.LayerNorm # called self.ln_post in VisualTransformer OpenCLIP
                normalized_shape: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
              - _target_: omnivore.models.pooling_helpers.SelectElement
                index: 0 # select CLS token
              - _target_: omnivision.model.model_init_utils.init_parameters
                model:
                  _target_: torch.nn.Linear
                  in_features: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
                  out_features: ${constants.head_final_embed_dim}
                  bias: False # OpenCLIP
                init_fns:
                  weight:
                    _target_: torch.nn.init.normal_
                    _partial_: True
                    mean: 0
                    std: 0.03608 # 768 ** -0.5
            fork_module: ""
            preprocessed_input_key: "vision_tokens"
            output_key: "vision_embed"
          - head:
              _target_: omnivore.models.pooling_helpers.SelectEOSAndProject
              proj:
                _target_: torch.nn.Sequential
                _args_:
                - _target_: torch.nn.LayerNorm # called self.ln_final in CLIP OpenCLIP
                  normalized_shape: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
                - _target_: omnivision.model.model_init_utils.init_parameters
                  model:
                    _target_: torch.nn.Linear
                    in_features: ${trainer.model.model.multimodal_model.trunks.0.trunk.embed_dim}
                    out_features: ${embed_dim}
                    bias: False # OpenCLIP
                  init_fns:
                    weight:
                      _target_: torch.nn.init.normal_
                      _partial_: True
                      mean: 0
                      std: 0.03608 # 768 ** -0.5
            fork_module: ""
            preprocessed_input_key: "text_tokens"
            output_key: "text_embed"
        postprocessors:
          - name: "normalize"
            postprocessor:
              _target_: torch.nn.Sequential
              _args_:
              - _target_: omnivore.models.helpers.EinOpsRearrange # unstack frames across batch dim.
                rearrange_expr: "(b t) d ->  b t d"
                b: ${constants.batch_size}
              - _target_: omnivore.models.helpers.EinOpsReduce # unstack frames across batch dim.
                reduce_expr: "b t d ->  b d"
                reduce_op: "mean"
              # - _target_: omnivore.models.helpers.AggregateVideoFrames
                # dim: 1              
              - _target_: omnivore.models.helpers.NormalizeAndCenter
                dim: -1
                out_features: ${constants.head_final_embed_dim}
          - name: "normalize_and_scale_text"
            postprocessor:
              _target_: torch.nn.Sequential
              _args_:
                - _target_: omnivore.models.helpers.NormalizeAndCenter
                  dim: -1
                  out_features: ${constants.head_final_embed_dim}
                - _target_: omnivore.models.helpers.LearnableLogitScaling
                  logit_scale_init: 10 # 1/0.1 as in SimCLR
                  learnable: False
        head_to_postprocessor:
          - input_key: "vision_embed"
            postprocessor_name: "normalize"
          - input_key: "text_embed"
            postprocessor_name: "normalize_and_scale_text"
  meters:
    val:
      k400:
        accuracy_top1:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
            top_k: 1
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
            top_k: 5
      audioset_video:
        mAP:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivore.meters.mean_avg_precision.MeanAvgPrecision
        # knn:
        #   _target_: omnivore.meters.knn_accuracy.KnnAccuracy
        #   feat_name: vision_embed
        #   topks: [10, 20]
        #   multilabel_mode: recall
        accuracy_top5:
          _target_: omnivore.meters.DictApplyMeterWrapper
          base_meter:
            _target_: omnivision.meters.accuracy_meter.AccuracyMeter
            top_k: 5
            multilabel_mode: recall

  loss:
    laion:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: vision_embed
      feat2_name: text_embed
      logit_scale_name: NULL
      normalize: False # OpenClip normalizes outputs in the model
    audioset:
      _target_: omnivore.losses.scaled_loss.ScaledLoss
      scale: 1.0
      loss_fn:
        _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
        feat1_name: vision_embed
        feat2_name: audio_embed
        logit_scale_name: NULL
        normalize: False # OpenClip normalizes outputs in the model

  distributed:
    backend: nccl
    comms_dtype: bfloat16
    find_unused_parameters: True

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir: ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10
    # tensorboard_embedding_writer:
    #   _target_: omnivore.logger.TensorBoardEmbeddingLogger
    #   path: ${..tensorboard_writer.log_dir}

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True

launcher:
  num_nodes: 1
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .


submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
