# @package _global_

base_batchsize_per_replica: 64

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 32
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.vision_text_dataset.VisionTextDataset
        base_dataset:
          _target_: omnivore.data.webdataset_helpers.WebVisionTextPipeline
          base_dataset_length: 400e6
          base_dataset_fn:
            _target_: omnivore.data.webdataset_helpers.get_wds_dataset
            _partial_: True
            resampled: True # needed for multi-node training
            urls:
              _target_: omnivore.utils.data.FileLoader.load
              return_idx: False
              path_list:
                - /checkpoint/imisra/datasets/laion/laion400m_tarlist.pkl
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  interpolation: 3
                  scale: [0.9, 1.0]
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
            base_transform:
              _target_: slip.tokenizer.SimpleTokenizer
              bpe_path_list:
                - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
                - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
      shuffle: True
      batch_size: ${base_batchsize_per_replica}
      num_workers: 12
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
        batch_kwargs:
          model_fwd_kwargs:
            vision_trunk:
              use_checkpoint: False
            text_trunk: {}
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer:
          _target_: slip.tokenizer.SimpleTokenizer
          bpe_path_list:
            - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
            - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
        label_names_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
        templates:
          _target_: omnivore.utils.data.FileLoader.load
          return_idx: False
          path_list:
            - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        base_dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          path_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
          label_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.Resize
                  size: 224
                  interpolation: 3
                - _target_: torchvision.transforms.CenterCrop
                  size: 224
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
      shuffle: False
      batch_size: ${base_batchsize_per_replica}
      num_workers: 12
      pin_memory: True
      drop_last: False
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL

  model:
    _target_: omnivore.models.openclip_model.MultiModalZeroShotEvalWrapperCLIP
    image_output_key: image_embed
    text_output_key: text_embed
    logit_scale_output_key: logit_scale
    clip_model:
      _target_: omnivore.models.openclip_model.CLIPShareAllLayersExceptSomeInResBlock
      embed_dim: 512
      vision_cfg:
        timm_model_name: NULL
        timm_model_pretrained: False
        timm_pool: NULL
        timm_proj: NULL
        image_size: 224
        patch_size: 14
        width: 1024
        layers: 24
        use_xformer: True
      text_cfg:
        context_length: 77
        vocab_size: 49408
        width: 1024
        heads: 16
        layers: 24
        use_xformer: True
    label_strings:
      in1k:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${trainer.data.val.dataset.tokenizer}
        label_names_file_list: ${trainer.data.val.dataset.label_names_file_list}
        templates: ${trainer.data.val.dataset.templates}

  optim:
      optimizer:
        _target_: torch.optim.AdamW
        betas:
          - 0.9
          - 0.98
        eps: 1e-6
      gradient_clip: NULL
      amp:
        enabled: True
        amp_dtype: float16 # bfloat16 or float16
      options:
        lr:
          - scheduler:
              _target_: fvcore.common.param_scheduler.CompositeParamScheduler
              schedulers:
                - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                  start_value: 1e-6
                  end_value: 5e-4  # from https://github.com/facebookresearch/SLIP#clip-vit-base-with-8-nodes-batch-size-4096
                - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                  start_value: ${..0.end_value}
                  end_value: 1e-6
              lengths: [0.00625, 0.99375]  # warm for 2440 iters; 1 epoch = 305 iter
              interval_scaling: ['rescaled', 'rescaled']
        weight_decay:
          - scheduler:
              _target_: fvcore.common.param_scheduler.ConstantParamScheduler
              value: 0.2
          - scheduler:
              _target_: fvcore.common.param_scheduler.ConstantParamScheduler
              value: 0.0
            param_names:
              - '*.bias'
              - '*.positional_embedding'
              - '*.class_embedding'
              - "*.logit_scale"
            module_cls_names: ['omnivore.models.openclip_model.LayerNorm']
  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
  loss:
    in1k:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: ${trainer.model.image_output_key}
      feat2_name: ${trainer.model.text_output_key}
      logit_scale_name: ${trainer.model.logit_scale_output_key}
      normalize: False # OpenClip normalizes outputs in the model



  distributed:
      backend: nccl
      comms_dtype: float16
      find_unused_parameters: False

  logging:
      tensorboard_writer:
        _target_: omnivore.logger.make_tensorboard_logger
        log_dir:  ${launcher.experiment_log_dir}/tensorboard
        flush_secs: 120
      log_dir: ${launcher.experiment_log_dir}/logs
      log_freq: 10

  checkpoint:
      save_dir: ${launcher.experiment_log_dir}/checkpoints
      save_freq: 1 # 0 only last checkpoint is saved.
      model_weight_initializer: NULL

  cuda:
      # https://pytorch.org/docs/stable/backends.html
      allow_tf32: True
      cudnn_deterministic: False
      cudnn_benchmark: True

launcher:
  num_nodes: 16
  gpus_per_node: 8

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
