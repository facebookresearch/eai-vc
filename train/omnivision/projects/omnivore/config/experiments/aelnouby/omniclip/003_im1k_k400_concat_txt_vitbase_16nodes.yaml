# @package _global_

base_batchsize_per_replica: 32

data_module:
  _target_: omnivision.data_module.base_data_module.BaseDataModule
  train:
    _target_: omnivore.data.concat_dataset.ConcatDataset
    max_steps: sum
    repeat_factors: [1.0, 1.0]
    datasets:
    - _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer:
          _target_: slip.tokenizer.SimpleTokenizer
          bpe_path_list:
            - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz  # AWS
            - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
              /vidpaths_train.npy
            - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
        label_names_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
        templates:
          _target_: omnivore.utils.data.FileLoader.load
          return_idx: False
          path_list:
            - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        base_dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          path_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/train_images_global.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_images_manifold_v2.npy
          label_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/train_labels.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_labels.npy
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.RandomResizedCrop
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.RandomHorizontalFlip
                  - _target_: omnivore.data.transforms.rand_auto_aug.RandAugment  # Essentially autoagument rand-m9-mstd0.5-inc1
                    magnitude: 9
                    magnitude_std: 0.5
                    increasing_severity: True
                  - _target_: torchvision.transforms.ColorJitter
                    brightness: 0.4
                    contrast: 0.4
                    saturation: 0.4
                    hue: 0.4
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.RandomErasing
                    p: .25
                  - _target_: torchvision.transforms.Normalize
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]
                  # - _target_: omnivore.data.transforms.image_video.ImageToSingleFrameVideo
      shuffle: True
      batch_size: ${base_batchsize_per_replica}
      num_workers: 10
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
        # TODO: Need to implement cutmix for vision-text
        # batch_transforms:
        # - _target_: omnivore.data.transforms.cutmixup.CutMixUp
        #   mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
        #   cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
        #   prob: 1.0 # probability of applying mixup or cutmix per batch or element
        #   switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
        #   mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
        #   correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
        #   label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
        #   num_classes: 1000 # number of classes for target
      worker_init_fn: NULL
    - _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer:
          _target_: slip.tokenizer.SimpleTokenizer
          bpe_path_list:
            - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
            - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
            - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
        label_names_file_list:
          - /checkpoint/aelnouby/datasets/k400_label_names.npy   # FAIR / AWS 
          - manifold://omnivore/tree/datasets/kinetics_400_meta/label_names.npy
        templates:
          _target_: omnivore.utils.data.FileLoader.load
          return_idx: False
          path_list:
            - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        base_dataset:
          _target_: omnivore.data.path_dataset.VideoPathDataset
          path_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics/400/vidpaths_train.npy
            - /checkpoint/aelnouby/datasets/k400/vidpaths_train.npy     # AWS
            - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_train.npy
          label_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics/400/labels_train.npy
            - /checkpoint/aelnouby/datasets/k400/labels_train.npy       # AWS
            - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_train.npy
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.RandomClipSampler
            clip_duration: 2
          frame_sampler:
            _target_: pytorchvideo.transforms.UniformTemporalSubsample
            num_samples: 16
          decoder: pyav
          normalize_to_0_1: True
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: pytorchvideo.transforms.ShortSideScale
                  size: 256
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                - _target_: torchvision.transforms.RandomHorizontalFlip
                  p: 0.5
                - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]

      shuffle: True
      batch_size: ${base_batchsize_per_replica}
      num_workers: 3
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: k400
        batch_kwargs:
          model_fwd_kwargs:
            use_checkpoint: True
          accum_steps: 4
      # batch_transforms:
      #   - _target_: omnivore.data.transforms.cutmixup.CutMixUp
      #     mixup_alpha: 0.8 # mixup alpha value, mixup is active if > 0.
      #     cutmix_alpha: 1.0 # cutmix alpha value, cutmix is active if > 0.
      #     prob: 1.0 # probability of applying mixup or cutmix per batch or element
      #     switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
      #     mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
      #     correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
      #     label_smoothing: 0.1 # apply label smoothing to the mixed target tensor
      #     num_classes: 400 # number of classes for target
      worker_init_fn: NULL
  val:
    _target_: omnivore.data.concat_dataset.ConcatDataset
    max_steps: sum
    datasets:
    - _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer: ${data_module.train.datasets.0.dataset.tokenizer}
        label_names_file_list: ${data_module.train.datasets.0.dataset.label_names_file_list}
        templates: ${data_module.train.datasets.0.dataset.templates}
        base_dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          path_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
          label_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.Resize
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.CenterCrop
                    size: 224
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]
      shuffle: False
      batch_size: ${base_batchsize_per_replica}
      num_workers: 10
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL
    - _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer: ${data_module.train.datasets.1.dataset.tokenizer}
        label_names_file_list: ${data_module.train.datasets.1.dataset.label_names_file_list}
        templates: ${data_module.train.datasets.1.dataset.templates}
        base_dataset:
          _target_: omnivore.data.path_dataset.VideoPathDataset
          path_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/vidpaths_val.npy
            - /checkpoint/aelnouby/datasets/k400/vidpaths_val.npy   # AWS
            - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
          label_file_list:
            - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/labels_val.npy
            - /checkpoint/aelnouby/datasets/k400/labels_val.npy     # AWS
            - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
          #name: k400_val
          clip_sampler:
            _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
            clip_duration: 10
            clips_per_video: 1
          frame_sampler:
            _target_: pytorchvideo.transforms.UniformTemporalSubsample
            num_samples: 160
          decoder: pyav
          normalize_to_0_1: True
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                - _target_: pytorchvideo.transforms.ShortSideScale
                  size: 224
                - _target_: torchvision.transforms._transforms_video.NormalizeVideo
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
                - _target_: omnivore.data.transforms.pytorchvideo.TemporalCrop
                  frames_per_clip: 16
                  stride: 40
                - _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
                  crop_size: 224
                  num_crops: 3

      # FIX shuffle false getting ignored
      shuffle: False
      batch_size: 4
      num_workers: 8
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: k400
        # batch_kwargs:
          # the omnivore public model expects an input_type key during forward
          # model_fwd_kwargs:
            # input_type: video
      worker_init_fn: NULL

lightning_module:
  _target_: omnivore.lightning_module.omnivore_lightning_module.OmnivoreLightningModule
  model:
    _target_: omnivision.model.model_wrappers.MultiModalZeroShotEvalWrapper
    logit_scale_output_key: "logit_scale"
    vision_trunk:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      handle_list_inputs: True
      trunk:
        _target_: omnivore.models.vision_transformer.VisionTransformer
        embed_dim: 768
        img_size:
          - 3
          - ${data_module.train.datasets.1.dataset.base_dataset.frame_sampler.num_samples}
          - 224
          - 224
        patch_size:
          - 2
          - 16
          - 16
        patch_embed_type: generic
        patch_embed_params_list:
          - _target_: omnivore.data.transforms.image_video.RepeatedPadIm2VideoSingleImage
            ntimes: 2
            time_dim: 2
          - _target_: torch.nn.Conv3d
            in_channels: 3
            out_channels: ${...embed_dim}
            kernel_size: ${...patch_size}
            stride:  ${...patch_size}
        depth: 12
        drop_path_rate: 0.1
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 12
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
      heads:
        - fork_module: ""
          head:
            _target_: torch.nn.Linear
            in_features: ${lightning_module.model.vision_trunk.trunk.embed_dim}
            out_features: 512
          input_key: NULL
          output_key: image_embed
      trunk_fields:
        - input_key: NULL
          args: ["vision"]
    text_trunk:
      _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
      trunk:
        _target_: omnivore.models.slip_text_transformer.SLIPTextTransformer
        context_length: 77
        vocab_size: 49408
        transformer_width: 512
        transformer_heads: 8
        transformer_layers: 12
      heads:
        - fork_module: ""
          head:
            _target_: torch.nn.Linear
            in_features: ${lightning_module.model.text_trunk.trunk.transformer_width}
            out_features: 512
          input_key: NULL
          output_key: text_embed
      trunk_fields:
        - input_key: NULL
          args: ["text"]
    label_strings:
      in1k:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${data_module.train.datasets.0.dataset.tokenizer}
        label_names_file_list: ${data_module.train.datasets.0.dataset.label_names_file_list}
        templates: ${data_module.train.datasets.0.dataset.templates}
      k400:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${data_module.train.datasets.0.dataset.tokenizer}
        label_names_file_list: ${data_module.train.datasets.1.dataset.label_names_file_list}
        templates: ${data_module.train.datasets.1.dataset.templates}
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      # betas: [0.9, 0.95]
      # eps: 1e-8
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 4e-4  # 5e-4 for 4K; 2x this LR for 512
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-6
            lengths: [0.0166, 0.9834]  # warm for 5 epochs
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
            # - 'logit_scale'
            - '*bias*'
            - '*.pos_embed'
            - '*.cls_token'
            # - '*ln*'
            # - 'visual*norm*'
          # TODO: allow other forms for class names
          module_cls_names: ['torch.nn.LayerNorm']
  metrics:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivore.metrics.accuracy.Accuracy
          top_k: 1
        accuracy_top5:
          _target_: omnivore.metrics.accuracy.Accuracy
          top_k: 5
      k400:
        accuracy_top1:
          _target_: omnivore.metrics.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivore.metrics.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
          top_k: 5
  loss:
    k400:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLossLegacy
      feat1_name: ${lightning_module.model.vision_trunk.heads.0.output_key}
      feat2_name: ${lightning_module.model.text_trunk.heads.0.output_key}
      learnable_temperature: True
      # logit_scale_name: ${lightning_module.model.logit_scale_output_key}
      all_gather_fn:
        _target_: slip.utils.all_gather_batch_with_grad
        _partial_: True
    in1k: ${.k400}

lightning_trainer:
  _target_: pytorch_lightning.Trainer
  num_nodes: ${launcher.num_nodes}
  gpus: ${launcher.gpus_per_node}
  sync_batchnorm: False
  replace_sampler_ddp: False
  max_epochs: 300
  accelerator: ${launcher.accelerator}
  strategy: ${launcher.strategy}

launcher:
  num_nodes: 16
  gpus_per_node: 8
  mode: train
  accelerator: gpu
  strategy: ddp
  every_n_epochs: 1

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  time: "72:00:00"
  mem: "470GB"
  constraints: "volta32gb"
  use_cluster: True
  cpus_per_task: 10
