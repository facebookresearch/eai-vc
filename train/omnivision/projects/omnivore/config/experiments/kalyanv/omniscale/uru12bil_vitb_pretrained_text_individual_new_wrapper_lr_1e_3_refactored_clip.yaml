# @package _global_

scratch:
  num_classes: 28052
  batch_size: 512
  phases_per_epoch: 500
  num_samples: 5000000000
  max_epochs: 1280
  num_nodes: 8
  english_threshold: 0.5
  context_length: 77
  patch_size: 16


trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: ${scratch.max_epochs}
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.fb.on_box_dataset.OnBoxDataset
      phase_type: "train"
      num_reader_worker_threads: 4
      num_python_transform_workers: 4
      max_num_dpp_servers_per_node: NULL # Defaults to 1 per GPU
      dpp_client_prefetch_capacity: NULL # TODO: set this. Defaults to numer of ddp workers
      dpp_server_glog_options:
        _target_: pytorch.data.fb.onbox.data_loader.GlogOptions
        disable_info_logs: True # TODO: Change this to True
      dataloader_setting:
        _target_: omnivore.data.fb.on_box_dataset.OnBoxDataLoaderSetting
        num_samples: ${scratch.num_samples}
        loader_mode: "WRAP_AROUND" # NORMAL, WRAP_AROUND, FULL_SYNC
        phases_per_epoch: ${scratch.phases_per_epoch}
        hive_shuffle_order: "NONE" # Change thils to SHUFFLE_ALL for wrap around.

      dataset:
        # Uru dataset with 5B images (~3.6B unique images)
        # be sure to name each version for experiment logging (like I used
        # `tool` for the Uru 10x10x10 runs) since the set of images and
        # hashtags changes with each regeneration
        _target_: omnivore.data.fb.hive_img_dataset.build_everstore_hivedataset
        batch_size: ${scratch.batch_size}
        table_name: omniscale_uru10x10_caption_cleaned_data_400m
        namespace: aml
        everstore_lookup_value: "everstore_handle"
        schema: ["everstore_handle", "raw_image", "labels", "id", "caption"]
        everstore_output_column: "raw_image"
        drop_incomplete: True
        partitions: NULL # Can be a list like  ["id=3", "colum_name=value"]
        batch_handler:
          _target_: omnivore.data.fb.hive_img_dataset.ImageTextBatchHiveHandler
          data_column: "raw_image"
          label_column: "labels"
          text_column: "caption"
          id_column: "id"
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
              - _target_: torchvision.transforms.RandomResizedCrop
                size: 224
                interpolation: 3
              - _target_: torchvision.transforms.RandomHorizontalFlip
              - _target_: torchvision.transforms.ToTensor
              - _target_: torchvision.transforms.Normalize
                mean:
                - 0.485
                - 0.456
                - 0.406
                std:
                - 0.229
                - 0.224
                - 0.225
          - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
              - _target_: omnivore.data.transforms.filter_captions.RemoveSpecialTokens
              - _target_: omnivore.data.transforms.filter_captions.FilterCaptionLanguage
                model_paths:
                - manifold://omniscale/tree/fasttext/lid.176.bin
                threshold: ${scratch.english_threshold}
                replace_by_hashtags: false
          - _target_: omnivore.data.transforms.filter_captions.ReplaceEmptyCaptionWithHashtags
          - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
            base_transform:
              _target_: omnivore.data.tokenizers.simple_tokenizer.SimpleTokenizer
              context_length: ${scratch.context_length}
              bpe_path_list:
              - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
          - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
            field: label
            base_transform:
              _target_: omnivore.data.fb.hive_img_dataset.StringToLabelList
              num_classes: ${scratch.num_classes}
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: in1k
            batch_kwargs:
              model_fwd_kwargs:
                vision_trunk:
                  use_checkpoint: True
                text_trunk: {}
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer:
          _target_: slip.tokenizer.SimpleTokenizer
          context_length: ${scratch.context_length}
          bpe_path_list:
            - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
            - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
        label_names_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
        templates:
          _target_: omnivore.utils.data.FileLoader.load
          return_idx: False
          path_list:
            - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        base_dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          path_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
          label_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.Resize
                  size: 224
                  interpolation: 3
                - _target_: torchvision.transforms.CenterCrop
                  size: 224
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
      shuffle: False
      batch_size: 64
      num_workers: 0
      pin_memory: True
      drop_last: False
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL

  model:
    _target_: omnivore.models.openclip_model.MultiModalZeroShotEvalWrapperCLIP
    image_output_key: image_embed
    text_output_key: text_embed
    logit_scale_output_key: logit_scale
    clip_model:
      _target_: slip.models.CLIP_V2
      embed_dim: 512
      freeze_vision: false
      freeze_text: true
      vision_width: 768
      text_model:
        _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
        strict: True
        state_dict:
          _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
          checkpoint_path: manifold://omniscale/tree/kalyanv/clip_oss_ckpts/ViT_B_16_state_dict.pt
          ckpt_state_dict_keys: []
          checkpoint_kernels:
          - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
            key_pattern:
            - "visual.*"
            - "logit_scale"
            - "input_resolution"
            - "context_length"
            - "vocab_size"
        model:
          _target_: slip.models.CLIPTextEncoder
          embed_dim: 512
          context_length: 77
          vocab_size: 49408
          transformer_width: 512
          transformer_heads: 8
          transformer_layers: 12
      # text_model:
      #   _target_: slip.models.CLIPTextEncoder
      #   embed_dim: 512
      #   context_length: 77
      #   vocab_size: 49408
      #   transformer_width: 512
      #   transformer_heads: 8
      #   transformer_layers: 12
      vision_model:
        _target_: omnivore.models.vision_transformer.VisionTransformer
        embed_dim: 768
        depth: 12
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 12
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
      # vision_model:
      #   _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
      #   strict: True
      #   state_dict:
      #     _target_: omnivore.models.checkpoint_utils.load_uru_official_checkpoint
      #     checkpoint_paths:
      #       - /checkpoint/qduval/uru/vit_b16.torch
      #       - manifold://omniscale/tree/uru/official_checkpoints/vit_b16.torch
      #   model:
      #     _target_: omnivore.models.vision_transformer.VisionTransformer
      #     embed_dim: 768
      #     depth: 12
      #     attn_target:
      #       _target_: omnivore.models.vision_transformer.Attention
      #       _partial_: True
      #       num_heads: 12
      #       proj_drop: 0
      #       qk_scale: NULL
      #       qkv_bias: True
      #       attn_drop: 0
    label_strings:
      in1k:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${trainer.data.val.dataset.tokenizer}
        label_names_file_list: ${trainer.data.val.dataset.label_names_file_list}
        templates: ${trainer.data.val.dataset.templates}

  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-8
    # gradient_logger: NULL
    # gradient_logger:
      # _target_: omnivore.optim.monitoring.GradientNormWatcher
      # output_path: ${launcher.experiment_log_dir}/gradients.json
      # accum_steps: 10
      # detailed: True
    gradient_clip: NULL
    # gradient_clip:
    #   _target_: torch.nn.utils.clip_grad_norm_
    #   _partial_: True
    #   max_norm: 1.0
    #   norm_type: 2.0
    amp:
      enabled: True
      amp_dtype: float16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-5
            lengths: [0.04, 0.96]
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.1
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names: # TODO: ln? blocks.norm1? blocks.norm2?, ln?
            - '*.bias*'
            - '*ln*'
            - "*logit_scale"
            - '*.pos_embed'
            - '*.cls_token'
            - 'clip_model.visual*norm*'
            - 'clip_model.visual.blocks.[0,1,2,3,4,5,6,7,8,9,10,11].norm1.weight'
            - 'clip_model.visual.blocks.[0,1,2,3,4,5,6,7,8,9,10,11].norm2.weight'
          module_cls_names: ['torch.nn.LayerNorm']

  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5

  loss:
    in1k:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: image_embed
      feat2_name: text_embed
      logit_scale_name: logit_scale

  distributed:
    backend: nccl
    comms_dtype: float16
    find_unused_parameters: true

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True


launcher:
  num_nodes: ${scratch.num_nodes}
  gpus_per_node: 8
  experiment_log_dir: ???

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
