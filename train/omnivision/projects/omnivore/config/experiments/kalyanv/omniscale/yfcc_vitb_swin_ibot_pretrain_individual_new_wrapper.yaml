# @package _global_

scratch:
  # CLIP training batch size is 32,768
  # This config is 16k (limited resources)
  batch_size: 1024
  num_nodes: 8

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 25
  mode: train
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.fb.yfcc_manifold_dataset.YFCCDatasetCLIPChunked
        root: manifold://omnivore/tree/datasets/yfcc100m/15mil_chunk_64
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  scale: [0.5, 1.0]
                #- _target_: torchvision.transforms.RandomHorizontalFlip
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.TextTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: omnivore.data.tokenizers.simple_tokenizer.SimpleTokenizer
                  bpe_path_list:
                    - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
                    - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
      collate_fn:
        _target_: omnivore.data.fb.yfcc_manifold_dataset.ListCollateWrapper
        collator:
          _target_: omnivore.data.api.DefaultOmnivoreCollator
          output_key: in1k
      shuffle: True
      batch_size: 4 # 4*64 (chunk size)
      num_workers: 4
      pin_memory: False
      drop_last: True

    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels
        tokenizer:
          _target_: slip.tokenizer.SimpleTokenizer
          bpe_path_list:
            - /checkpoint/imisra/datasets/SLIP/bpe_simple_vocab_16e6.txt.gz
            - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
        label_names_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/classnames_zs.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/classnames_zs.npy
        templates:
          _target_: omnivore.utils.data.FileLoader.load
          return_idx: False
          path_list:
            - /checkpoint/imisra/datasets/in1k_disk/templates_openai.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/templates_openai.npy
        base_dataset:
          _target_: omnivore.data.path_dataset.ImagePathDataset
          path_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
          label_file_list:
            - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
            - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
          transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.Resize
                  size: 224
                  interpolation: 3
                - _target_: torchvision.transforms.CenterCrop
                  size: 224
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
      shuffle: False
      batch_size: 64
      num_workers: 12
      pin_memory: True
      drop_last: False
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL

  model:
    _target_: omnivore.models.openclip_model.MultiModalZeroShotEvalWrapperCLIP
    image_output_key: image_embed
    text_output_key: text_embed
    logit_scale_output_key: logit_scale
    clip_model:
      _target_: slip.models.CLIP
      embed_dim: 512
      vision_width: 1024
      context_length: 77
      vocab_size: 49408
      transformer_width: 512
      transformer_heads: 8
      transformer_layers: 12
      freeze_vision: true
      vision_model:
        _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
        state_dict:
          _target_: omnivision.model.checkpoint_utils.load_vissl_checkpoint_trunk_only
          path_list:
          - manifold://omnivore/tree/imisra/configs/experiments/pretrain/ibot/39_ibot_swinB_LS_12crops_fz5ep_mim70_tw50_wdcos_ep300_wd.sweep/0/checkpoint.torch
        model:
          _target_: omnivore.models.swin_transformer_VISSL_DEBUG.SwinTransformer3D
          attn_drop_rate: 0.0
          depths:
          - 2
          - 2
          - 18
          - 2
          drop_path_rate: 0.5
          drop_rate: 0.0
          embed_dim: 128
          layer_scale_init_value: 1.0e-06
          layer_scale_type: per_channel
          masked_image_modeling: true
          mlp_ratio: 4.0
          norm_layer_eps: 1.0e-06
          num_heads:
          - 4
          - 8
          - 16
          - 32
          patch_norm: true
          patch_size:
          - 1
          - 4
          - 4
          pretrained: null
          pretrained2d: false
          qk_scale: null
          qkv_bias: true
          window_size:
          - 1
          - 7
          - 7
    label_strings:
      in1k:
        _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
        tokenizer: ${trainer.data.val.dataset.tokenizer}
        label_names_file_list: ${trainer.data.val.dataset.label_names_file_list}
        templates: ${trainer.data.val.dataset.templates}

  # model:
  #   _target_: omnivision.model.model_wrappers.MultiModalZeroShotEvalWrapper
  #   learnable_logit_scale: True #TODO: Check with Rohit
  #   vision_trunk:
  #     _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
  #     trunk:
  #       _target_: omnivore.models.helpers.NoGradWrapper
  #       module:
  #         _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
  #         state_dict:
  #           _target_: omnivision.model.checkpoint_utils.load_vissl_checkpoint_trunk_only
  #           path_list:
  #             - manifold://omnivore/tree/imisra/configs/experiments/pretrain/ibot/39_ibot_swinB_LS_12crops_fz5ep_mim70_tw50_wdcos_ep300_wd.sweep/0/checkpoint.torch
  #         model:
  #           _target_: omnivore.models.swin_transformer_VISSL_DEBUG.SwinTransformer3D
  #           attn_drop_rate: 0.0
  #           depths:
  #           - 2
  #           - 2
  #           - 18
  #           - 2
  #           drop_path_rate: 0.5
  #           drop_rate: 0.0
  #           embed_dim: 128
  #           layer_scale_init_value: 1.0e-06
  #           layer_scale_type: per_channel
  #           masked_image_modeling: true
  #           mlp_ratio: 4.0
  #           norm_layer_eps: 1.0e-06
  #           num_heads:
  #           - 4
  #           - 8
  #           - 16
  #           - 32
  #           patch_norm: true
  #           patch_size:
  #           - 1
  #           - 4
  #           - 4
  #           pretrained: null
  #           pretrained2d: false
  #           qk_scale: null
  #           qkv_bias: true
  #           window_size:
  #           - 1
  #           - 7
  #           - 7

  #     heads:
  #       - fork_module: ""
  #         head:
  #           _target_: torch.nn.Linear
  #           in_features: 1024
  #           out_features: 512
  #         input_key: in1k
  #         output_key: image_embed
  #     trunk_fields:
  #       - input_key: NULL
  #         args: ["vision"]
  #   text_trunk:
  #     _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
  #     trunk:
  #       _target_: omnivore.models.slip_text_transformer.SLIPTextTransformer
  #       context_length: 77
  #       vocab_size: 49408
  #       transformer_width: 512
  #       transformer_heads: 8
  #       transformer_layers: 12
  #     heads:
  #       - fork_module: ""
  #         head:
  #           _target_: torch.nn.Linear
  #           in_features: 512
  #           out_features: 512
  #         input_key: in1k
  #         output_key: text_embed
  #     trunk_fields:
  #       - input_key: NULL
  #         args: ["text"]
  #   label_strings:
  #     in1k:
  #       _target_: omnivore.data.path_dataset.PathDatasetWithTextLabels.gen_label_strings
  #       tokenizer: ${trainer.data.val.dataset.tokenizer}
  #       label_names_file_list: ${trainer.data.val.dataset.label_names_file_list}
  #       templates: ${trainer.data.val.dataset.templates}
  optim:
    optimizer:
      _target_: torch.optim.AdamW
      betas:
        - 0.9
        - 0.98
      eps: 1e-8
    # gradient_logger: NULL
    # gradient_logger:
      # _target_: omnivore.optim.monitoring.GradientNormWatcher
      # output_path: ${launcher.experiment_log_dir}/gradients.json
      # accum_steps: 10
      # detailed: True
    gradient_clip: NULL
    # gradient_clip:
    #   _target_: torch.nn.utils.clip_grad_norm_
    #   _partial_: True
    #   max_norm: 1.0
    #   norm_type: 2.0
    amp:
      enabled: True
      amp_dtype: float16 # bfloat16 or float16
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 1e-3
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 1e-5
            lengths: [0.04, 0.96]
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.1
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names: # TODO: ln? blocks.norm1? blocks.norm2?, ln?
            - '*.bias*'
            - '*ln*'
            - "*logit_scale"
            - 'clip_model.visual*norm*'
          module_cls_names: ['torch.nn.LayerNorm']

  meters:
    val:
      in1k:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5

  loss:
    in1k:
      _target_: omnivore.losses.contrastive_loss.ContrastiveLoss
      feat1_name: image_embed
      feat2_name: text_embed
      logit_scale_name: logit_scale

  distributed:
    backend: nccl
    comms_dtype: float16
    find_unused_parameters: true

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer: NULL

  cuda:
    # https://pytorch.org/docs/stable/backends.html
    allow_tf32: True
    cudnn_deterministic: False
    cudnn_benchmark: True


launcher:
  num_nodes: ${scratch.num_nodes}
  gpus_per_node: 8
  experiment_log_dir: ???

hydra:
  output_subdir: NULL
  run:
    dir: .

submitit:
  name: clip_base
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
