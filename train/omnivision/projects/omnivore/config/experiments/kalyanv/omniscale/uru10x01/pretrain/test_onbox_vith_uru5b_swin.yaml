# @package _global_

scratch:
  # number of classes in the `tool` version of Uru 5B
  num_classes: 29000 #27999
  batch_size: 2
  phases_per_epoch: 2
  num_samples: 10000

trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 2
  accelerator: cuda
  seed_value: 123
  mode: train

  data:
    train:
      _target_: omnivore.data.fb.on_box_dataset.OnBoxDataset
      phase_type: "train"
      num_reader_worker_threads: 4
      num_python_transform_workers: 4
      max_num_dpp_servers_per_node: NULL # Defaults to 1 per GPU
      dpp_client_prefetch_capacity: NULL # TODO: set this. Defaults to numer of ddp workers
      dpp_server_glog_options:
        _target_: pytorch.data.fb.onbox.data_loader.GlogOptions
        disable_info_logs: True # TODO: Change this to True
      dataloader_setting:
        _target_: omnivore.data.fb.on_box_dataset.OnBoxDataLoaderSetting
        num_samples: ${scratch.num_samples}
        loader_mode: "WRAP_AROUND" # NORMAL, WRAP_AROUND, "FULL_SYNC"
        phases_per_epoch: ${scratch.phases_per_epoch}
        hive_shuffle_order: "NONE" # NONE, Change thils to SHUFFLE_ALL for wrap around.

      dataset:
        # Uru dataset with 5B images (~3.6B unique images)
        # be sure to name each version for experiment logging (like I used
        # `tool` for the Uru 10x10x10 runs) since the set of images and
        # hashtags changes with each regeneration
        _target_: omnivore.data.fb.hive_img_dataset.build_everstore_hivedataset
        batch_size: ${scratch.batch_size}
        table_name: omniscale_uru10x10_caption_cleaned_data_trunc10k
        namespace: aml
        everstore_lookup_value: "everstore_handle"
        schema: ["everstore_handle", "raw_image", "labels", "id"]
        everstore_output_column: "raw_image"
        drop_incomplete: True
        partitions: NULL # Can be a list like  ["id=3", "colum_name=value"]
        batch_handler:
          _target_: omnivore.data.fb.hive_img_dataset.ImageBatchHiveHandler
          data_column: "raw_image"
          label_column: "labels"
          id_column: "id"
          transforms:
            - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  - _target_: torchvision.transforms.RandomResizedCrop
                    size: 224
                    interpolation: 3
                  - _target_: torchvision.transforms.RandomHorizontalFlip
                  - _target_: torchvision.transforms.ToTensor
                  - _target_: torchvision.transforms.Normalize
                    mean: [0.485, 0.456, 0.406]
                    std: [0.229, 0.224, 0.225]
            - _target_: omnivore.data.transforms.transform_wrappers.SingleFieldTransform
              field: label
              base_transform:
                _target_: torchvision.transforms.Compose
                transforms:
                  # "c1,c2,...,cN" --> torch.Tensor([c1, c2, ..., cN], dtype=int)
                  - _target_: omnivision.utils.generic.csv_str_to_int_tensor
                    _partial_: True
                  # (N,) --> (1, N)
                  - _target_: torch.unsqueeze
                    dim: 0
                    _partial_: True
                  # (1, N) --> (N, C) 1-hot vector
                  - _target_: omnivision.utils.generic.convert_to_one_hot
                    classes:  ${scratch.num_classes}
                    _partial_: True
                  # (N, C) to (C,)
                  - _target_: torch.sum
                    dim: 0
                    _partial_: True
                  # Need a float output to work with BCELoss
                  - _target_: omnivision.utils.generic.change_dtype
                    dtype: "float"
                    _partial_: True
          collate_fn:
            _target_: omnivore.data.api.DefaultOmnivoreCollator
            output_key: image
            batch_kwargs:
              model_fwd_kwargs:
                use_checkpoint: True

  model:
    _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
    trunk:
      _target_: omnivore.models.swin_transformer.SwinTransformer3D
      pretrained: NULL
      pretrained2d: False
      patch_size: [1, 4, 4]
      embed_dim: 96
      depths: [2, 2, 6, 2]
      num_heads: [3, 6, 12, 24]
      window_size: [1, 7, 7]
      mlp_ratio: 4.
      qkv_bias: True
      qk_scale: NULL
      drop_rate: 0.
      attn_drop_rate: 0.
      drop_path_rate: 0.2
      patch_norm: True
    heads:
      - head:
          _target_: omnivision.model.model_init_utils.init_parameters
          model:
            _target_: torch.nn.Linear
            in_features: 768  # 8 * 96
            out_features: ${scratch.num_classes}
          init_fns:
            weight:
              _target_: torch.nn.init.normal_
              _partial_: True
              mean: 0
              std: 0.01
            bias:
              _target_: torch.nn.init.zeros_
              _partial_: True
        fork_module: ""
        input_key: NULL
        output_key: NULL
    trunk_fields:
      - input_key: NULL
        args: ["vision"]

  optim:
    optimizer:
      _target_: torch.optim.AdamW
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 10e-7
                end_value: 10e-4
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: 10e-4
                end_value: 10e-6
            lengths: [0.07, 0.93]
            interval_scaling: ['rescaled', 'rescaled']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
             - '*.bias'
             #- '*.pos_embed'
             #- '*.cls_token'
             #- '*.absolute_pos_embed'
             - '*.relative_position_bias_table'
             #- '*.norm'
          module_cls_names: ['torch.nn.LayerNorm']

  meters:
    train:
      image:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
  loss:
    image:
      # can just copy paste this implementation to our code
      _target_: omnivore.losses.soft_target_cross_entropy_loss.SoftTargetCrossEntropyLoss

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.


launcher:
  num_nodes: 4
  gpus_per_node: 8
  experiment_log_dir: ???
