# @package _global_
# Reproducing MAE w/ ViT-Base trunk
# configs/experiments/pretrain/ibot_mae/001_mae_vitbase_decoder8L512_400ep.yaml (81.8%)

defaults:
  - /experiments/kalyanv/no_lightning/v0.2/base.yaml
  - _self_

launcher:
  gpus_per_node: 8
  num_nodes: 8


trainer:
  max_epochs: 400

  data:
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImagePathDataset
        path_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/train_images_global.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_images_manifold_v2.npy
        label_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/train_labels.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/train_labels.npy
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  scale: [0.2, 1.0]
                  interpolation: 3
                - _target_: torchvision.transforms.RandomHorizontalFlip
                  p: 0.5
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.MaskingTransform
            masking_object:
              _target_: omnivore.data.transforms.mask_image_modeling.MaskImageModeling
              pred_ratio: 0.75
              pred_ratio_var: 0.0
              pred_shape:
                _target_: omnivore.data.transforms.mask_image_modeling.RandMasking
              patch_size:
                - 1
                - ${trainer.model.trunk.patch_size}
                - ${trainer.model.trunk.patch_size}
      shuffle: True
      batch_size: 64
      num_workers: 10
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImagePathDataset
        path_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/val_images_global.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_images_manifold_v2.npy
        label_file_list:
          - /checkpoint/imisra/datasets/in1k_disk/val_labels.npy
          - manifold://omnivore/tree/datasets/imagenet_1k_meta/val_labels.npy
        #name: imagenet1k_val
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: 224
                  scale: [0.2, 1.0]
                  interpolation: 3
                - _target_: torchvision.transforms.RandomHorizontalFlip
                  p: 0.5
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.MaskingTransform
            masking_object:
              _target_: omnivore.data.transforms.mask_image_modeling.MaskImageModeling
              pred_ratio: 0.75
              pred_ratio_var: 0.0
              pred_shape:
                _target_: omnivore.data.transforms.mask_image_modeling.RandMasking
              patch_size:
                - 1
                - ${trainer.model.trunk.patch_size}
                - ${trainer.model.trunk.patch_size}
      shuffle: False
      batch_size: 64
      num_workers: 8
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: in1k
      worker_init_fn: NULL

  model:
    _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
    trunk:
      _target_: omnivore.models.vision_transformer.VisionTransformer
      embed_dim: 768
      depth: 12
      patch_size: 16
      attn_target:
        _target_: omnivore.models.vision_transformer.Attention
        _partial_: True
        num_heads: 12
        proj_drop: 0
        qk_scale: NULL
        qkv_bias: True
        attn_drop: 0
      learnable_pos_embed: False  # Use sinusoidal positional encoding
      masked_image_modeling: True
      patch_dropping: True
      decoder:
        _target_: omnivore.models.vision_transformer.Decoder
        _partial_: True
        embed_dim: ${trainer.model.trunk.embed_dim}
        decoder_depth: 8
        decoder_embed_dim: 512
        learnable_pos_embed: False  # Use sinusoidal positional encoding
        attn_target:
          _target_: omnivore.models.vision_transformer.Attention
          _partial_: True
          num_heads: 16
          proj_drop: 0
          qk_scale: NULL
          qkv_bias: True
          attn_drop: 0
    heads:
    - head:
        _target_: omnivore.models.heads.mae_head.MAEHead
        in_features: ${trainer.model.trunk.decoder.decoder_embed_dim}
        out_features: 768
      input_key: NULL
      output_key: NULL
      fork_module: ""
      # TODO: Add init
    trunk_fields:
      - input_key: NULL
        args: ["vision"]
        kwargs: {"mask": "mask"}

  optim:
    gradient_clip: NULL
    amp:
      enabled: False
      amp_dtype: float16 # bfloat16 or float16

    optimizer:
      _target_: torch.optim.AdamW
      betas: [0.9, 0.95]
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CompositeParamScheduler
            schedulers:
              - _target_: fvcore.common.param_scheduler.LinearParamScheduler
                start_value: 1e-6
                end_value: 2.4e-3 # batch size of 512
              - _target_: fvcore.common.param_scheduler.CosineParamScheduler
                start_value: ${..0.end_value}
                end_value: 0.0
            lengths: [0.1, 0.9]  # warm for 40 epochs
            interval_scaling: ['rescaled', 'fixed']
      weight_decay:
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.05
        - scheduler:
            _target_: fvcore.common.param_scheduler.ConstantParamScheduler
            value: 0.0
          param_names:
              - '*.bias'
              - '*.cls_token'
          module_cls_names: ['torch.nn.LayerNorm']

  meters: NULL

  loss:
    in1k:
      _target_: omnivore.losses.mae_loss.MAELoss
      norm_pix_loss: True
      norm_pix_per_channel: False
      patch_size: ${trainer.model.trunk.patch_size}
