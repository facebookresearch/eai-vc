# @package _global_

data_module:
  _target_: omnivision.data_module.base_data_module.BaseDataModule
  val:
    dataset:
      _target_: omnivore.data.path_dataset.VideoPathDataset
      path_file_list:
        - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/vidpaths_val.npy
        - manifold://omnivore/tree/datasets/kinetics_400_meta/vidpaths_val.npy
      label_file_list:
        - /checkpoint/rgirdhar/Work/FB/2021/003_JointImVid/Datasets/Kinetics_lowres/400/labels_val.npy
        - manifold://omnivore/tree/datasets/kinetics_400_meta/labels_val.npy
      #name: k400_val
      clip_sampler:
        _target_: pytorchvideo.data.clip_sampling.ConstantClipsPerVideoSampler
        clip_duration: 10
        clips_per_video: 1
      frame_sampler:
        _target_: pytorchvideo.transforms.UniformTemporalSubsample
        num_samples: 40 # 5 clips of 8x8
      decoder: pyav
      normalize_to_0_1: True
      transform:
        _target_: torchvision.transforms.Compose
        transforms:
        - _target_: pytorchvideo.transforms.ShortSideScale
          size: 224
        - _target_: torchvision.transforms._transforms_video.NormalizeVideo
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
        - _target_: omnivore.data.transforms.pytorchvideo.TemporalCrop
          frames_per_clip: 8
          stride: 8
        - _target_: omnivore.data.transforms.pytorchvideo.SpatialCrop
          crop_size: 224
          num_crops: 3
    shuffle: False
    batch_size: 1
    num_workers: 4
    pin_memory: True
    drop_last: True
    collate_fn:
      _target_: omnivore.data.api.DefaultOmnivoreCollator
    worker_init_fn: NULL

lightning_module:
  _target_: omnivore.lightning_module.language_vision_lightning_module.LanguageVisionModule
  tokenizer:
    _target_: slip.tokenizer.SimpleTokenizer
    bpe_path_list:
    - /checkpoint/kalyanv/data/slip/bpe_simple_vocab_16e6.txt.gz
    - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/bpe_simple_vocab_16e6.txt.gz
  labels:
    _target_: omnivore.utils.data.load_file_from_list
    file_list:
    - /checkpoint/kalyanv/data/slip/labels.json
    - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/labels.json
  templates:
    _target_: omnivore.utils.data.load_file_from_list
    file_list:
    - /checkpoint/kalyanv/data/slip/templates.json
    - manifold://omnivore/tree/datasets/yfcc100m/meta_data/yfcc_meta_data/templates.json
  dataset_name: kinetics400_frames
  meters:
    val:
      accuracy_top1:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 1
      accuracy_top5:
        _target_: omnivore.meters.avg_pooled_accuracy_list_meter.AvgPooledAccuracyListMeter
        top_k: 5
  loss: NULL

lightning_trainer:
  _target_: pytorch_lightning.Trainer
  num_nodes: ${launcher.num_nodes}
  gpus: ${launcher.gpus_per_node}
  log_gpu_memory: null
  sync_batchnorm: False
  replace_sampler_ddp: False
  limit_train_batches: 2
  limit_test_batches: 0
  max_epochs: 1
  accelerator: ${launcher.accelerator}
  strategy: ${launcher.strategy}

launcher:
  num_nodes: 1
  gpus_per_node: 2
  mode: val
  accelerator: gpu
  strategy: ddp

submitit:
  name: k400_zero_shot
  partition: learnlab
  time: "72:00:00"
  mem: "470GB"
  constraints: "volta32gb"
  use_cluster: True
  cpus_per_task: 10
