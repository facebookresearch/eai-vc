# @package _global_


scratch:
  num_classes: 1000
  num_nodes: 1
  train_batch_size: 64
  val_batch_size: 128
  image_size: 224
  patch_size: 16


trainer:
  _target_: omnivore.trainer.omnivision_trainer.OmnivisionTrainer
  max_epochs: 28
  accelerator: cuda
  seed_value: 123
  val_epoch_freq: 1
  mode: train

  ema:
    enabled: True
    decay: 1e-4
    warmup: 0.05

  data:
    _target_: omnivision.data_module.base_data_module.BaseDataModule
    train:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImagePathDataset
        new_prefix: /checkpoint/omniscale_oss/datasets/imagenet_1k/
        path_file_list:
          - /checkpoint/omniscale_oss/datasets_meta/in1k/train_images.npy
        label_file_list:
          - /checkpoint/omniscale_oss/datasets_meta/in1k/train_labels.npy
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.RandomResizedCrop
                  size: ${scratch.image_size}
                  interpolation: 3
                - _target_: torchvision.transforms.RandomHorizontalFlip
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.MaskingTransform
            masking_object:
              _target_: omnivore.data.transforms.mask_image_modeling.MaskForPerformance
              patch_size: ${scratch.patch_size}
      shuffle: True
      batch_size: ${scratch.train_batch_size}
      num_workers: 8
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: image
        batch_transforms:
        - _target_: omnivore.data.transforms.cutmixup.CutMixUp
          mixup_alpha: 0.1 # mixup alpha value, mixup is active if > 0.
          cutmix_alpha: 0.0 # cutmix alpha value, cutmix is active if > 0.
          prob: 1.0 # probability of applying mixup or cutmix per batch or element
          switch_prob: 0.5 # probability of switching to cutmix instead of mixup when both are active
          mode: batch # how to apply mixup/cutmix params (per 'batch', 'pair' (pair of elements), 'elem' (element)
          correct_lam: True # apply lambda correction when cutmix bbox clipped by image borders
          label_smoothing: 0.0 # apply label smoothing to the mixed target tensor
          num_classes: ${scratch.num_classes} # number of classes for target
      worker_init_fn: NULL
    val:
      _target_: omnivore.data.torch_dataset.TorchDataset
      dataset:
        _target_: omnivore.data.path_dataset.ImagePathDataset
        new_prefix: /checkpoint/omniscale_oss/datasets/imagenet_1k/
        path_file_list:
          - /checkpoint/omniscale_oss/datasets_meta/in1k/val_images.npy
        label_file_list:
          - /checkpoint/omniscale_oss/datasets_meta/in1k/val_labels.npy
        transforms:
          - _target_: omnivore.data.transforms.transform_wrappers.VisionTransform
            base_transform:
              _target_: torchvision.transforms.Compose
              transforms:
                - _target_: torchvision.transforms.Resize
                  size: ${scratch.image_size}
                  interpolation: 3
                - _target_: torchvision.transforms.CenterCrop
                  size: ${scratch.image_size}
                - _target_: torchvision.transforms.ToTensor
                - _target_: torchvision.transforms.Normalize
                  mean: [0.485, 0.456, 0.406]
                  std: [0.229, 0.224, 0.225]
          - _target_: omnivore.data.transforms.transform_wrappers.MaskingTransform
            masking_object:
              _target_: omnivore.data.transforms.mask_image_modeling.WithEmptyMask
              patch_size: 16
      shuffle: False
      batch_size: ${scratch.val_batch_size}
      num_workers: 8
      pin_memory: True
      drop_last: True
      collate_fn:
        _target_: omnivore.data.api.DefaultOmnivoreCollator
        output_key: image
      worker_init_fn: NULL

  model:
    _target_: omnivision.model.model_wrappers.MIMOHeadWrapper
    trunk:
      _target_: omnivore.models.vision_transformer.VisionTransformer
      patch_size: ${scratch.patch_size}
      img_size: ${scratch.image_size}
      embed_dim: 1024
      depth: 24
      drop_path_rate: 0.0
      patch_dropping: True
      attn_target:
        _target_: omnivore.models.vision_transformer.Attention
        _partial_: True
        num_heads: 16
        proj_drop: 0
        qk_scale: NULL
        qkv_bias: True
        attn_drop: 0
    heads:
      - head:
          _target_: omnivision.model.model_init_utils.init_parameters
          model:
            _target_: torch.nn.Linear
            in_features: 1024
            out_features: ${scratch.num_classes}
          init_fns:
            weight:
              _target_: torch.nn.init.zeros_
              _partial_: True
            bias:
              _target_: torch.nn.init.zeros_
              _partial_: True
        fork_module: ""
        input_key: NULL
        output_key: NULL
    trunk_fields:
      - input_key: NULL
        args: ["vision"]
        kwargs: {"mask": "mask"}
  optim:
    amp:
      enabled: True
      amp_dtype: bfloat16
    optimizer:
      _target_: torch.optim.SGD
      lr: 0  # SGD expects an lr arg during initialization
      weight_decay: 0
      nesterov: True
      momentum: 0.9
    options:
      lr:
        - scheduler:
            _target_: fvcore.common.param_scheduler.CosineParamScheduler
            start_value: 0.006
            end_value: 0
  meters:
    train:
      image:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
    val:
      image:
        accuracy_top1:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 1
        accuracy_top5:
          _target_: omnivision.meters.accuracy_meter.AccuracyMeter
          top_k: 5
  loss:
    image:
      _target_: torch.nn.CrossEntropyLoss

  logging:
    tensorboard_writer:
      _target_: omnivore.logger.make_tensorboard_logger
      log_dir:  ${launcher.experiment_log_dir}/tensorboard
      flush_secs: 120
    log_dir: ${launcher.experiment_log_dir}/logs
    log_freq: 10

  checkpoint:
    save_dir: ${launcher.experiment_log_dir}/checkpoints
    save_freq: 0 # 0 only last checkpoint is saved.
    model_weight_initializer:
      _partial_: True
      _target_: omnivision.model.checkpoint_utils.load_state_dict_into_model
      strict: False # heads aren't loaded
      state_dict:
        _target_: omnivision.model.checkpoint_utils.load_checkpoint_and_apply_kernels
        checkpoint_path: '/checkpoint/omniscale_ugc/qduval/omnivision_omnivore/config/experiments/qduval/rsc/clip/clip_vit_l16_ig5b_bs32k_captions_slip_uru_hashtags_n16.yaml/0/checkpoints/checkpoint.pt'
        ckpt_state_dict_keys: ["model"]
        checkpoint_kernels:
          - _target_: omnivision.model.checkpoint_utils.CkptRenameKeysKernel
            source_pattern: "clip_model.visual."
            target_pattern: "trunk."
          - _target_: omnivision.model.checkpoint_utils.CkptExcludeKernel
            key_pattern:
            - "clip_model.*"


launcher:
  num_nodes: ${scratch.num_nodes}
  gpus_per_node: 8
  experiment_log_dir: ???


submitit:
  partition: learnlab
  timeout_hour: 72
  use_cluster: True
  cpus_per_task: 12
  port_range: [10000, 65000]
